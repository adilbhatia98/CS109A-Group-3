<!DOCTYPE html>

<html lang="en-us">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge">

  <title>Home - Trump Tweet VIX</title>
  <link rel="stylesheet" href="http://localhost:4000/assets/css/just-the-docs.css">
  
  <script type="text/javascript" src="http://localhost:4000/assets/js/vendor/lunr.min.js"></script>
  
  <script type="text/javascript" src="http://localhost:4000/assets/js/just-the-docs.js"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
</head>


  <div class="page-wrap">
    <div class="side-bar">
      <a href="http://localhost:4000" class="site-title fs-6 lh-tight">Trump Tweet VIX</a>
      <span class="fs-3"><button class="js-main-nav-trigger navigation-list-toggle btn btn-outline" type="button" data-text-toggle="Hide">Menu</button></span>
      <div class="navigation main-nav js-main-nav">
        <nav>
  <ul class="navigation-list">
    
    
      
        
          <li class="navigation-list-item active">
            
            <a href="http://localhost:4000/" class="navigation-list-link active">Home</a>
            
          </li>
        
      
    
      
        
          <li class="navigation-list-item">
            
            <a href="http://localhost:4000/background.html" class="navigation-list-link">Background</a>
            
          </li>
        
      
    
      
        
          <li class="navigation-list-item">
            
            <a href="http://localhost:4000/final_notebook/data.html" class="navigation-list-link">Data Exploration</a>
            
          </li>
        
      
    
      
        
          <li class="navigation-list-item">
            
            <a href="http://localhost:4000/final_notebook/models.html" class="navigation-list-link">Models</a>
            
          </li>
        
      
    
      
        
          <li class="navigation-list-item">
            
            <a href="http://localhost:4000/conclusions.html" class="navigation-list-link">Conclusions</a>
            
          </li>
        
      
    
  </ul>
</nav>

      </div>
      <footer role="contentinfo" class="site-footer">
        <p class="text-small text-grey-dk-000 mb-0">This site uses <a href="https://github.com/pmarsceill/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.</p>
      </footer>
    </div>
    <div class="main-content-wrap">
      <div class="page-header">
        <div class="main-content">
          
          <div class="search js-search">
            <div class="search-input-wrap">
              <input type="text" class="js-search-input search-input" placeholder="Search Trump Tweet VIX" aria-label="Search Trump Tweet VIX" autocomplete="off">
              <svg width="14" height="14" viewBox="0 0 28 28" xmlns="http://www.w3.org/2000/svg" class="search-icon"><title>Search</title><g fill-rule="nonzero"><path d="M17.332 20.735c-5.537 0-10-4.6-10-10.247 0-5.646 4.463-10.247 10-10.247 5.536 0 10 4.601 10 10.247s-4.464 10.247-10 10.247zm0-4c3.3 0 6-2.783 6-6.247 0-3.463-2.7-6.247-6-6.247s-6 2.784-6 6.247c0 3.464 2.7 6.247 6 6.247z"/><path d="M11.672 13.791L.192 25.271 3.02 28.1 14.5 16.62z"/></g></svg>
            </div>
            <div class="js-search-results search-results-wrap"></div>
          </div>
          
          
            <ul class="list-style-none text-small mt-md-1 mb-md-1 pb-4 pb-md-0 js-aux-nav aux-nav">
              
                <li class="d-inline-block my-0"><a href="https://github.com/adilbhatia98/CS109A-Group-3">Trump Tweet VIX on GitHub</a></li>
              
            </ul>
          
        </div>
      </div>
      <div class="main-content">
        
          
        
        <div class="page-content">
          <h1 class="no_toc" id="models">Models</h1>

<h2 class="no_toc text-delta" id="table-of-contents">Table of contents</h2>

<ol id="markdown-toc">
  <li><a href="#setup" id="markdown-toc-setup">Setup</a></li>
  <li><a href="#baseline-model" id="markdown-toc-baseline-model">Baseline Model - Logistic</a></li>
  <li><a href="#l1-and-l2-regularization" id="markdown-toc-l1-and-l2-regularization">L1 and L2 Regularization</a></li>
  <li><a href="#random-forest" id="markdown-toc-random-forest">Random Forest</a></li>
  <li><a href="#boosting" id="markdown-toc-boosting">Boosting</a></li>
  <li><a href="#neural-networks" id="markdown-toc-neural-networks">Neural Networks</a></li>
  <li><a href="#model-selection" id="markdown-toc-model-selection">Model Selection</a></li>
</ol>

<hr />

<h1 id="setup">Setup</h1>
       
<p>Our goal for this project is to build a model that can accurately predict changes in VIX prices after Trump tweets. In the model, we use characteristics of the VIX data (like <code class="highlighter-rouge">Last Price</code>) and multiple characteristics of the Twitter data. Also, some predictors are common to both datasets, such as date/time predictors.</p>
          
<p>Based on these predictors, we initially considered two types of models. First, we considered one that predicts absolute price changes in VIX pricing (continuous outcome). Second, we considered one that predicts the sign of VIX pricing changes (positive, negative, or no change - a categorical outcome). Intuitively, given what we know about the VIX and its pricing fluctuations, we thought it made more sense to focus on a model that predicts the change in pricing. Thus, we built a model that has a categorical output variable: -1 for negative price delta, 0 for no price delta, 1 for positive price delta.</p>
  
<p>We also wanted to create interval versions of this model that looks at the VIX price change over 1 minute, 5, 10, 20 , 30, and 60 minutes. We wanted to consider all these options to determine the most accurate and most useful model. Realistically, if our model predicts well 10+ minutes after the tweet, it can offer a great chance to earn positive returns by trading on the VIX index. Another important consideration is the threshold at which we declare an outcome positive or negative. Given we are using minute-by-minute data, we decided upon a 0.001 threshold so that any change between -0.001 and 0.001 inclusive should be categorized as 0 (no price delta). To explain why we chose 0.001, there are a couple points to consider. First, any change is very helpful in terms of playing the market through trading, even if it is a small change. Second, a larger threshold could have potentially forced our models to disproportionately predict 0 change (at all time intervals), which would not be a useful model. As we will see below, at larger intervals, our models predict far fewer 0 outcomes, becoming more successful at predicting positive or negative changes in the VIX pricing over time.</p>  

<p>For each model trial, we first split our data into a train and test set, so that we are later able to assess how well our model performs in both a train set and a not-seen test set. Realistically, training accuracy reflects how well a given model 'understands' the data it is presented with, and testing accuracy reflects how well that model can be generalized to accurately form predictions about data it has not yet seen. For each time interval model, we drop the irrelevant time-based price predictors. For example, model30 does not include <code class="highlighter-rouge">price_delta_30</code> or <code class="highlighter-rouge">price_delta_60</code> as predictors. We perform this drop for each time interval model.</p>
          
<h1 id="baseline-model">Baseline Model - Logistic</h1>

<p>Our baseline model represents a simple logistic regression with a multiclass outcome variable. Using the predictors in our data, the model predicts the 'change in VIX price' classification as positive, negative, or 0 (no change) using a simple logistic regression.</p>          
         
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 1 min</span>
<span class="n">logreg0</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train0</span><span class="p">,</span> <span class="n">y_train0</span><span class="p">)</span>
<span class="n">logreg_fit_train0</span> <span class="o">=</span> <span class="n">logreg0</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train0</span><span class="p">)</span>
<span class="n">logreg_fit_test0</span> <span class="o">=</span> <span class="n">logreg0</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test0</span><span class="p">)</span>

<span class="n">train_scores_logreg0</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train0</span><span class="p">,</span> <span class="n">logreg_fit_train0</span><span class="p">)</span>
<span class="n">test_scores_logreg0</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test0</span><span class="p">,</span> <span class="n">logreg_fit_test0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training Accuracy 1 min: &quot;</span><span class="p">,</span> <span class="n">train_scores_logreg0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Testing Accuracy 1 min: &quot;</span><span class="p">,</span> <span class="n">test_scores_logreg0</span><span class="p">)</span>

<span class="c1"># 5 min</span>
<span class="n">logreg5</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train5</span><span class="p">,</span> <span class="n">y_train5</span><span class="p">)</span>
<span class="n">logreg_fit_train5</span> <span class="o">=</span> <span class="n">logreg5</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train5</span><span class="p">)</span>
<span class="n">logreg_fit_test5</span> <span class="o">=</span> <span class="n">logreg5</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test5</span><span class="p">)</span>

<span class="n">train_scores_logreg5</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train5</span><span class="p">,</span> <span class="n">logreg_fit_train5</span><span class="p">)</span>
<span class="n">test_scores_logreg5</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test5</span><span class="p">,</span> <span class="n">logreg_fit_test5</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training Accuracy 5 min: &quot;</span><span class="p">,</span> <span class="n">train_scores_logreg5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Testing Accuracy 5 min: &quot;</span><span class="p">,</span> <span class="n">train_scores_logreg5</span><span class="p">)</span>

<span class="c1"># 10 min</span>
<span class="n">logreg10</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train10</span><span class="p">,</span> <span class="n">y_train10</span><span class="p">)</span>
<span class="n">logreg_fit_train10</span> <span class="o">=</span> <span class="n">logreg10</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train10</span><span class="p">)</span>
<span class="n">logreg_fit_test10</span> <span class="o">=</span> <span class="n">logreg10</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test10</span><span class="p">)</span>

<span class="n">train_scores_logreg10</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train10</span><span class="p">,</span> <span class="n">logreg_fit_train10</span><span class="p">)</span>
<span class="n">test_scores_logreg10</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test10</span><span class="p">,</span> <span class="n">logreg_fit_test10</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training Accuracy 10 min: &quot;</span><span class="p">,</span> <span class="n">train_scores_logreg10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Testing Accuracy 10 min: &quot;</span><span class="p">,</span> <span class="n">train_scores_logreg10</span><span class="p">)</span>

<span class="c1"># 20 min</span>
<span class="n">logreg20</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train20</span><span class="p">,</span> <span class="n">y_train20</span><span class="p">)</span>
<span class="n">logreg_fit_train20</span> <span class="o">=</span> <span class="n">logreg20</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train20</span><span class="p">)</span>
<span class="n">logreg_fit_test20</span> <span class="o">=</span> <span class="n">logreg20</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test20</span><span class="p">)</span>

<span class="n">train_scores_logreg20</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train20</span><span class="p">,</span> <span class="n">logreg_fit_train20</span><span class="p">)</span>
<span class="n">test_scores_logreg20</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test20</span><span class="p">,</span> <span class="n">logreg_fit_test20</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training Accuracy 20 min: &quot;</span><span class="p">,</span> <span class="n">train_scores_logreg20</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Testing Accuracy 20 min: &quot;</span><span class="p">,</span> <span class="n">train_scores_logreg20</span><span class="p">)</span>

<span class="c1"># 30 min</span>
<span class="n">logreg30</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train30</span><span class="p">,</span> <span class="n">y_train30</span><span class="p">)</span>
<span class="n">logreg_fit_train30</span> <span class="o">=</span> <span class="n">logreg30</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train30</span><span class="p">)</span>
<span class="n">logreg_fit_test30</span> <span class="o">=</span> <span class="n">logreg30</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test30</span><span class="p">)</span>

<span class="n">train_scores_logreg30</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train30</span><span class="p">,</span> <span class="n">logreg_fit_train30</span><span class="p">)</span>
<span class="n">test_scores_logreg30</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test30</span><span class="p">,</span> <span class="n">logreg_fit_test30</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training Accuracy 30 min: &quot;</span><span class="p">,</span> <span class="n">train_scores_logreg30</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Testing Accuracy 30 min: &quot;</span><span class="p">,</span> <span class="n">train_scores_logreg30</span><span class="p">)</span>

<span class="c1"># 60 min</span>
<span class="n">logreg60</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train60</span><span class="p">,</span> <span class="n">y_train60</span><span class="p">)</span>
<span class="n">logreg_fit_train60</span> <span class="o">=</span> <span class="n">logreg60</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train60</span><span class="p">)</span>
<span class="n">logreg_fit_test60</span> <span class="o">=</span> <span class="n">logreg60</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test60</span><span class="p">)</span>

<span class="n">train_scores_logreg60</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train60</span><span class="p">,</span> <span class="n">logreg_fit_train60</span><span class="p">)</span>
<span class="n">test_scores_logreg60</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test60</span><span class="p">,</span> <span class="n">logreg_fit_test60</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training Accuracy 60 min: &quot;</span><span class="p">,</span> <span class="n">train_scores_logreg60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Testing Accuracy 60 min: &quot;</span><span class="p">,</span> <span class="n">train_scores_logreg60</span><span class="p">)</span>
</pre></div>

    </div>
</div>
  
<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Interval</th>
      <th>training accuracy</th>
      <th>test accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>4</th>
      <td>30 minute</td>
      <td>0.523760</td>
      <td>0.503306</td>
    </tr>
    <tr>
      <th>3</th>
      <td>20 minute</td>
      <td>0.515702</td>
      <td>0.500826</td>
    </tr>
    <tr>
      <th>5</th>
      <td>60 minute</td>
      <td>0.534946</td>
      <td>0.499586</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10 minute</td>
      <td>0.491529</td>
      <td>0.476033</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5 minute</td>
      <td>0.476033</td>
      <td>0.452893</td>
    </tr>
    <tr>
      <th>0</th>
      <td>1 minute</td>
      <td>0.468182</td>
      <td>0.451240</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
          
<h1 id="l1-and-l2-regularization">L1 and L2 Regularization</h1>

<p>We then decided to incorporate regularization in an attempt to improve our logistic model's predictive ability. Lasso regularization (l1) sets the effects/coefficients of unimportant predictors to 0, whereas ridge (l2) simply minimizes/lowers those effects.</p>               
          
<h1 id="random-forest">Random Forest</h1>

<p>Our first ensemble method is random forest, which randomly subsets predictors upon which to generate decision trees. We tested out a few different tree depth and number parameters ourselves and determined that a depth of 5 and number of trees of 100 was ideal for our analysis.</p>


<h1 id="boosting">Boosting</h1>
<p>Next, we considered boosting, an iterative approach that might eliminate some more of the error from above. Note that the RF model is based on fully grown trees, meaning it is susceptible to low bias and high variance in its results. Boosting, however is based on 'weaker' learners meaning high bias and low variance.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># define classifier function</span>
<span class="k">def</span> <span class="nf">boostingClassifier</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">depth</span><span class="p">):</span>
    <span class="c"># AdaBoostClassifier</span>
    <span class="n">abc</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">depth</span><span class="p">),</span>
                         <span class="n">n_estimators</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">)</span>
    <span class="n">abc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="c"># staged_score train to plot</span>
    <span class="n">abc_predicts_train</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">abc</span><span class="o">.</span><span class="n">staged_score</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">abc_predicts_train</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">"train"</span><span class="p">);</span>

    <span class="c"># staged_score test to plot</span>
    <span class="n">abc_predicts_test</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">abc</span><span class="o">.</span><span class="n">staged_score</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">abc_predicts_test</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">"test"</span><span class="p">);</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"AdaBoost Classifier Accuracy, n = "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">depth</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Iterations"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
    <span class="k">return</span><span class="p">(</span><span class="s">"Maximum test accuracy for depth of "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">depth</span><span class="p">)</span><span class="o">+</span><span class="s">" is "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">abc_predicts_test</span><span class="p">))</span><span class="o">+</span><span class="s">" at "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">abc_predicts_test</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">abc_predicts_test</span><span class="p">)))</span><span class="o">+</span><span class="s">" iterations"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">boostingClassifier</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span>
</code></pre></div></div>

<p><img src="/final_notebook/output_60_0.png" alt="png" /></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Maximum test accuracy for depth of 1 is 0.9150197628458498 at 773 iterations
</code></pre></div></div>

<p><img src="/final_notebook/output_60_2.png" alt="png" /></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Maximum test accuracy for depth of 2 is 0.9298418972332015 at 751 iterations
</code></pre></div></div>

<p><img src="/final_notebook/output_60_4.png" alt="png" /></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Maximum test accuracy for depth of 3 is 0.9268774703557312 at 500 iterations
</code></pre></div></div>

<p><img src="/final_notebook/output_60_6.png" alt="png" /></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Maximum test accuracy for depth of 4 is 0.9219367588932806 at 530 iterations
</code></pre></div></div>

<p>We see based upon an AdaBoostClassifier the maximum test accuracy of 93.0% is attained at a depth of 2. This is attained after 751 iterations. The AdaBoostClassifier is our most accurate model so far.</p>

<h1 id="neural-networks">Neural Networks</h1>

<p>Finally, we created an artificial neural network to classify our outcomes.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># check input and output dimensions</span>
<span class="n">input_dim_2</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">output_dim_2</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">print</span><span class="p">(</span><span class="n">input_dim_2</span><span class="p">,</span><span class="n">output_dim_2</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>14 1
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># create sequential multi-layer perceptron</span>
<span class="n">model2</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span> 

<span class="c"># initial layer</span>
<span class="n">model2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim_2</span><span class="p">,</span>  
                <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span> 

<span class="c"># second layer</span>
<span class="n">model2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim_2</span><span class="p">,</span>  
                <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>

<span class="c"># third layer</span>
<span class="n">model2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim_2</span><span class="p">,</span>  
                <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>

<span class="c"># output layer</span>
<span class="n">model2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">))</span>

<span class="c"># compile the model</span>
<span class="n">model2</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">'sgd'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
<span class="n">model2</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># fit the model</span>
<span class="n">model2_history</span> <span class="o">=</span> <span class="n">model2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">validation_split</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># model loss</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Neural Net - Model 1] Loss: "</span><span class="p">,</span> <span class="n">model2_history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Neural Net - Model 1] Val Loss: "</span><span class="p">,</span> <span class="n">model2_history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Neural Net - Model 1] Test Loss: "</span><span class="p">,</span> <span class="n">model2</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Neural Net - Model 1] Accuracy: "</span><span class="p">,</span> <span class="n">model2_history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'acc'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Neural Net - Model 1] Val Accuracy: "</span><span class="p">,</span> <span class="n">model2_history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_acc'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Neural Net - Model 1] Loss:  7.79790558079957
[Neural Net - Model 1] Val Loss:  8.034205742033103
[Neural Net - Model 1] Test Loss:  [7.719139232937055, 0.5158102769154334]
[Neural Net - Model 1] Accuracy:  0.5108695654529828
[Neural Net - Model 1] Val Accuracy:  0.49604743024106085
</code></pre></div></div>

<p>Our initial accuracy isn’t great. We achieve an accuracy of 48.9% in the training and 50.4% in the validation, and an accuracy of 48.4% in the test. Let’s see if we can improve our network to fit the data better.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># create sequential multi-layer perceptron</span>
<span class="n">model3</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span> 

<span class="c"># Hidden layers</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">40</span><span class="p">):</span>
    <span class="n">model3</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim_2</span><span class="p">,</span> 
        <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span> 

<span class="c"># output layer</span>
<span class="n">model3</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">output_dim_2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">))</span>


<span class="c"># compile the model</span>
<span class="n">model3</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'acc'</span><span class="p">])</span>
<span class="n">model3</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># fit the model</span>
<span class="n">model3_history</span> <span class="o">=</span> <span class="n">model3</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">validation_split</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># model loss</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Neural Net - Model 2] Loss: "</span><span class="p">,</span> <span class="n">model3_history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Neural Net - Model 2] Val Loss: "</span><span class="p">,</span> <span class="n">model3_history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Neural Net - Model 2] Test Loss: "</span><span class="p">,</span> <span class="n">model3</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Neural Net - Model 2] Accuracy: "</span><span class="p">,</span> <span class="n">model3_history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'acc'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Neural Net - Model 2] Val Accuracy: "</span><span class="p">,</span> <span class="n">model3_history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_acc'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Neural Net - Model 2] Loss:  0.6267417644590524
[Neural Net - Model 2] Val Loss:  0.6291195959220698
[Neural Net - Model 2] Test Loss:  [0.6115785545040026, 0.6432806319398843]
[Neural Net - Model 2] Accuracy:  0.625857809154077
[Neural Net - Model 2] Val Accuracy:  0.6197530875971288
</code></pre></div></div>

<p>Even after changing hyperparameters, our neural network does not perform very well. Using 40 layers and 300 epochs, the accuracy in the training data is still 62.8% while the accuracy in the test is 65.2%. This is baffling, because we expected our neural network to perform very well. Perhaps this mediocre perforance is due to limitations of our data set (only 14 features and &lt;5000 songs), or of the specific methods we used.</p>

<hr />

<h1 id="model-selection">Model Selection</h1>

<p>Based upon the presented analysis, we conclude that our boosted decision tree classifier, at a depth of 2 with 751 iterations, is the best model. It achieves the highest accuracy in the test set, of 93.0%.</p>



          
        </div>
      </div>
    </div>
  </div>
</html>

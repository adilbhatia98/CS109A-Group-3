{
  "0": {
    "id": "0",
    "title": "Background",
    "content": "Background  Project Question  Project Motivation Approach Literature Review How do President Trump’s tweets impact the volatility of the S&P 500? How has that impact changed over time, if at all, since he began his term as U.S. President? In September 2019, J.P. Morgan launched their ‘Volfefe’ index meant to track U.S. treasury bond volatility based on market sentiment changes due to unpredictable tweets from President Trump. Using a portion of Trump’s tweets since 2017, JPM developed a model that predicted bond market volatility surprisingly accurately. In our project, we examine VIX, the first benchmark index to measure expectations of future market volatility. VIX prices are based on S&P 500 options, and rapid and/or significant price changes usually reflect sudden changes in market sentiment. For example, if VIX jumps 10% in a given day, it suggests that investors are generally more unsure about the future of the U.S. economy, resulting in greater volatility in the overall market. We had three goals with this project: create a well-performing model that can predict changes in VIX based on Trump’s tweets use that model to generate positive returns in the coming future produce a clean interface using jekyll to display our project analysis and results so that others can replicate our method and hopefully improve it! Approach UPDATE We first downloaded Trump’s twitter database and cleaned the data to focus on the tweet content. Then, we analyzed it using textblob to generate a sentiment score for each tweet. Next, we went to HBS and manually pulled minute-by-minute VIX data (a cumbersome process, but one we believe was worth the effort!). Then, we consolidated this data into a single dataframe. We then randomly split this data into a training and test set with the outcome variable being the change in VIX pricing and the core explanatory variable being the sentiment score for a given tweet, along with several other predictors constructed based on the twitter and VIX data. Predicting the absolute VIX price yielded irrelevant results (nonsurprisingly), but predicting the change in VIX price produced some interesting results. Next, we built and fit a variety of classifiers with differing VIX price change time intervals. Models we built include: Baseline Model Logistic Classifier For each model, we evaluated its accuracy on both our training and and test set. Based on accuracy scores, we determined the classifier with the highest performance on the test set. Finally, we ran our best-performing model on a fresh set of songs and asked Grace if she liked her new playlist. Literature Review Using Twitter to Predict the Stock Market Attemt to extract mood levels from Social Media applications in order to predict stock returns Uses roughly 100 million tweets that were published in Germany between January, 2011 and November, 2013 Find that it is necessary to take into account the spread of mood states among Internet users Portfolio increases by up to 36 % within a sixmonth period after the consideration of transaction costs Data Mining Twitter To Predict Stock Market Movements Sentiment analysis of Twitter data from July through December, 2013 Attempt to find correlation between users’ sentiments and NASDAQ closing price and trading volume Find that “Happy” and “sad” sentiment variables’ lags are strongly correlated with closing price and “excited” and “calm” lags are strongly correlated with trading volume Incorporates interesting word weighting to classify tweets into emotional groupings A topic-based sentiment analysis model to predict stock market price movement using Weibo mood Demonstrate that the emotions automatically extracted from the large scale Weibo posts represent the real public opinions about some special topics of the stock market in China. Nonlinear autoregressive model, using neural network with exogenous sentiment inputs is proposed to predict the stock price movement. “Given the performance, if more related topics for a stock are found out, the accuracy could be higher considering more completed topic-based sentiment inputs.” Recognize that the time lag on sentiment makes it difficult to fully predict market changes Can Twitter Help Predict Firm-Level Earnings and Stock Returns? Test whether opinions of individuals tweeted just prior to a firm’s earnings announcement predict its earnings and announcement returns Find that the aggregate opinion from individual tweets successfully predicts a firm’s forthcoming quarterly earnings and announcement returns",
    "url": "http://localhost:4000/background.html",
    "relUrl": "/background.html"
  },
  "1": {
    "id": "1",
    "title": "Conclusions",
    "content": "Conclusions TABLE OF CONTENTS Analysis of Results Extending Our Model Limitations and Future Work Analysis of Results Our best model is the boosted decision tree classifier with a depth of 2 and 751 iterations, which performs with an accuracy of 95.4% in the training set and 93.0% in the test set. The following table summarizes the accuracies for all our models, ordered by accuracy in the test set: Model Type	Train Accuracy	Test Accuracy Naive Model	50.3%	51.6% Neural Network	62.8%	65.2% kNN	63.1%	65.9% Logistic Regression With L2 Regularization	69.2%	66.9% Baseline Logistic Regression	69.4%	67.1% QDA	86.6%	86.7% LDA	88.1%	88.4% Logistic Regression With L1 Regularization	88.6%	88.7% Decision Tree Classifier	88.0%	89.0% Decision Tree Classifier With Bagging	93.6%	91.8% Random Forest	92.9%	92.0% Boosted Decision Tree Classifier	95.4%	93.0% Our lowest performing models include the logistic regression with the naive model, the neural network, and the kNN model. Our best performing models were all ensemble methods. The boosted decision tree classifier, the random forest model, and the decision tree classifier with bagging performed best. We tuned the parameters and hyperparameters of each base model to maximize the accuracy score of each, which leads us to believe that we achieved the maximum possible classification accuracy given the constraints of our dataset. Indeed, this model performs much better than the baseline model, which achieves an accuracy of only 51.6% in the test set. Finally, while usually time and space are considerations when evaluating different types of models, because they do not constrain our original problem, we chose to focus on accuracy. However, a qualitative assessment of these metrics determined that all models were comparable in terms of runtime and memory use with the exception of the neural nets that took additional time. One consideration to note is that the boosted model is less suited for parallelization than other ensemble methods, because it is iterative. However, the runtime was less than a few seconds, so we will prioritize the accuracy of the model over this concern. — Extending Our Model We can now try to generate a playlist customized to Grace’s taste using our chosen model. We will present the model with a list of songs that both Grace and the model have not seen before. We’ll then have the model assess whether these songs should be included in the playlist and then verify that with Grace’s opinion. # load in dataset full_songs_df = pd.read_csv("data/spotify-test.csv") # drop unnecessary columns songs_df = full_songs_df.drop(columns=['type', 'id', 'uri', 'track_href', 'analysis_url', 'name', 'artist', 'Unnamed: 0']) # recreating the best model best_abc = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=800, learning_rate = 0.05) best_abc.fit(x_train, y_train) predictions = best_abc.predict(songs_df) print("Songs Selected for Grace's Playlist") for i in range(len(predictions)): if predictions[i] == 1: print(full_songs_df.loc[i]['name']) Songs Selected for Grace's Playlist Never Seen Anything "Quite Like You" - Acoustic Crazy World - Live from Dublin Whatever You Do Come on Love 1,000 Years Machine After Dark Sudden Love (Acoustic) Georgia I Don't Know About You This randomly selected dataset had 26 songs. These songs had never been classified by Grace before and our best model (the boosted decision tree classifier with a depth of 2) was used to predict whether songs would be included in her playlist. We then played all the songs in the dataset to Grace to see whether she would include them in her playlist. The model performed accurately, except for one song which she said she would not have added to her playlist (“I Don’t Know About You”). One reason for this mishap could be that our model isn’t 100% accurate, so this song could be by chance one of the ones it messes up; 1 missed song out of 26 is reasonable for a model with 93% accuracy. Another reason could be that Grace’s actual taste is different from how she made the playlist (perhaps she is in a different emotive or environmental state that temporally affects her preferences, or perhaps her underlying preferences have changed). Despite this error, overall, Grace was pleased that we could use data science to automate her playlist selection procees! Limitations and Future Work Data Size We generated a dataset by consolidating a large array of songs that vary in genre, language, tempo, rhythm, etc. We tried to curate a dataset that mimicked the variety of songs that Spotify has. Grace then had to go through these songs and classify whether she would like them in her playlist or not. Due to a multitude of constraints, we only had 5000 songs between both and training and test data. Ideally more songs that accurately capture the variety of songs that Spotify has would improve the training procedures for models. Data Inclusion Outside of the side of the data set, there are other data sets that can be used discover new predictors or variables about songs. We can explore lyrics for example and see how that contributes to a model’s recommendations. Thus requires us expanding beyond the SpotifyAPI and exploring other data sets. Adapting Playlists This entire project was built off of the preferences of one individual: Grace. While this proved to be a good proof of concept, future exploration should be done to analyze how the best model can help create playlists for others based upon their interests. Collaborative Filtering Collaborative filtering is another type of data modeling that is commonly used for recommendation algorithms. It is based on the fundamental idea that people perfer things similar to the things they’ve established they like. As such, it would be a good model to further investigate for this given project. Improve Neural Network Our neural network did not perform particularly well. While we tuned many hyperparameters, further tuning, exploring other network structures, and changing optimizers may help improve our network. Additionally, we could consider using convolutional neural networks. Dynamic Preferences Finally, a review of the literature highlighted the importance of dynamic preferences. Individuals often adapt the music they want to listen to at a particular time based on their emotions or external situation. Allowing for a modification of models to include these parameters could be useful. For example, if Grace tracked the time of day that she added each song to her playlist and we could use that time as a feature, we could better suggest songs suited to a particular time of day. This could help adjust for temporal effects, such as desiring certain music during a morning run or commute to work, versus desiring different music for an evening shower, versus desiring different music for a party late at night.",
    "url": "http://localhost:4000/conclusions.html",
    "relUrl": "/conclusions.html"
  },
  "2": {
    "id": "2",
    "title": "Data Exploration",
    "content": "Data Exploration TABLE OF CONTENTS Data Collection and Cleaning Twitter VIX Consolidated Data Data Description Exploratory Data Analysis Data Collection and Cleaning Twitter We pulled all of Trump’s tweets in the last few years from his Twitter archive. Due to the naature of the database, we spent much time cleaning this data for our purposes. We took the following steps to clean the data: Importing raw data from the archive Removing unnecessary columns Adjusting GMT to Eastern Time + accounting for daylight savings Manually fixing errors in cells where the delimiting was incorrectly done in the database output and manually re-inserting the delimiting character First, we removed all Twitter data preceding June 1, 2016. Trump became the presumptive nominee of the Republican Party that summer, and we figured that this represented the beginning of Trump tweets’ meaningful market relevance. There remained numerous instances of tweet data from the database lumping together multiple tweets in a single entry. We noticed that when this occurred, the components of these tweets were delimited by the character “{“. We used Excel’s “find” function to filter for all such cases. We then transferred all these “mega-cells” into a separate Excel sheet, which automatically separated the subcomponents of the tweet data: each row represented the data, including the actual tweet string, for a given tweet. However, the various classes of data (e.g., text, id_str, date, etc.) were lumped together in one column. These were split using the aforementioned delimiter. We appended these cleaned data into the original Excel sheet, and then sorted the entire sheet by date to complete the process. Additionally, tweets that were subject to the above issue—and many others, in general—also misclassified retweets as Trump’s original tweets. Retweets started with ‘@[handle] : ‘ so I wrote a function to filter through the data for entries starting with ‘@‘ that also included a ‘:’. Not all such instances were retweets—some were just tweets by Trump—and so this process had a manual component too. We utilized ntlk’s textblob function in order to analyze the sentiment of tweets in our data set. For each tweet, this function created a polarity score (the more positive a tweet is, the closer the score is to 1; the more negative, the closer it is to -1). The function also returns a subjectivity score. Lower subjectivity score means that the tweet’s polarity score more objectively represents its sentiment. VIX VIX is the first benchmark index to measure expectations of future market volatility (based on S&P 500 options). Since Milestone 2, we have procured minute-by-minute VIX data from 12/2015 - 11/11/2019. This data includes only VIX pricing on trading days throughout the past few years (excludes holidays, weekends, etc.). We manually pulled the data from a Bloomberg Terminal at HBS Baker Library. Given the size of the dataset and the Terminal download limits, we manually copied and pasted the VIX data directly into a csv file. Note that all other sources of VIX data are at best day-by-day and typically cost a nontrivial amount. VIX is managed by CBOE (Chicago Board Options Exchange). The global trading hours for VIX can be found here. Trading hours range from 3:15 am EST until 4:15 pm EST. There is a break bewteen 9:15 and 9:30 am, but this is addressed in how we consolidate our data. Consolidated Data In order to consolidate the data, we merge our Twitter and VIX dataframes on date/time to ensure that we are only looking at tweets that are posted during VIX trading hours. That way, we do not have to worry about tweets occurring outside these hours. Also, when we look at the change in VIX price, we are only looking at changes during trading hours, so examining only tweets that are posted during trading hours allows us to perform this analysis soundly. from textblob import TextBlob tweets = twitter_archive_df['text'] tweets_list = [tweet for tweet in tweets] big_tweet_string = ' '.join(tweets_list) tokens = word_tokenize(big_tweet_string) words = [word.lower() for word in tokens if word.isalpha()] stop_words = set(stopwords.words('english')) words = [word for word in words if not word in stop_words] scores = [] subjectivity = [] for tweet in tweets_list: blob_tweet = TextBlob(tweet) sentiment = blob_tweet.sentiment score = sentiment[0] subject = sentiment[1] scores.append(float(score)) subjectivity.append(float(subject)) twitter_archive_df['sent_score'] = scores twitter_archive_df['subjectivity'] = subjectivity # twitter_archive_df.head() Data Description Our data includes the following features: price_delta, price_delta_5, … , price_delta_60: These variables assisted us in the model-building process, by contributing to our understanding of how VIX digested the information contained within a Trump tweet. price_delta was the next-minute change in the VIX, price_delta_5 was the change 5 minutes following the tweet, and so on. Naturally, these cannot be predictors in our model. However, by including these as features in the random forest, and then assessing their importance, we were able to determine what portion of VIX movement, say, 15 minutes following a Trump tweet could be explained by the index’s movement in the first 10 minuets. We ultimately determined that the market absorbs the impact of the tweet within the first 5-10 minutes of the tweet’s posting. That is to say, price_delta_5 was a hugely important predictor of price_delta_10. This suggested to us that our predictive efforts should seek to predict the VIX’s change after >10 minutes have passed, so as to allow the impact of the tweet to register. (But not too much longer, for then confounding factors may enter the model.) Last Price: This variable represents the last recorded price in the VIX index at the time of the tweet. is_retweet: This variable describes whether a given tweet is a retweet or not. We re-coded the boolean from the database as a binary value, with is_retweet = 1 representing a tweet that Trump retweeted, while zero denotes a Trump tweet. This variable is important because we might expect Trump’s own tweets to have a greater market impact than his retweets. 24_hr_perc, 7_day_perc, 14_day_perc, 30_day_perc: These variables describe the change in the value of VIX over the preceding 24hrs, 7 days, 14 days and 30 days. The intuition behind including this is that the direction and strength of volatility’s momentum could well be tied to the market impact of a tweet. If, for instance, volatility has dropped because the market has become somewhat complacent about trade-related risks (i.e., the percentage change in VIX is negative), then a Trump tweet voicing dissatisfaction about the progress of a trade deal would cause a much larger spike in volatility than if markets had already “priced in” the possibility of antagonistic chatter from Trump. minutes: This variable is the minute in the day that the tweet was made. There is research that shows the majority of trading is done near the market’s opening and close. From this follows that we may expect tweet’s made around this time to have greater predictive impact than those made during the market’s slower hours. hour_0 to hour_23: These are dummy variables for the hour in the day that the tweet was made. The intuition for this is similar to that for ‘minutes’; perhaps the former is too granular, in which case the hour dummies might have more meaningful predictivity. month_1 to month_12: These are dummy variables for the month in which the tweet was posted, with month_1 representing January. Perhaps some months are more susceptible to market volatility, perhaps due to increased trading volumes or perhaps because those months are heavy with corporate earnings. That would potentially magnify the impact of tweets made in those months. year: This is the year in which the tweet was posted (after escalation of trade war, after inaugration). This would be a proxy for tweets made, say, before and after the escalation of trade war, or before and after the inaugration. We would expect that the impact of Trump tweets would vary depending on the circumstances of the particular year. keywords (contained in all_keywords): These are all words that could be associated with the US-China trade war. By creating a column for each keyword, we turn each one into a predictor, allowing us to assess the impact on the VIX index of the presence of that particular keyword. Each row in a given column notes the number of times that particular keyword is used in that tweet. This has been coded to be case-insensitive–important given Trump’s occasionally unconventional capitalization pracatices. The usage of one of these trade-related keywords could make a Trump tweet particularly meaningful to the market. keyword_mentions: This is the total number of non-unique keyword mentions in a given tweet. That is to say, if ‘trade’ is mentioned 2 time, ‘China’ 3 times and ‘deficit’ 1 time, then keyword_mentions=6. We think it is plausible that increased used of keywords would be associated with a tweet that contains more substance on a potentially market-moving matter. handle_mentions: This is the number of other Twitter users mentioned in the tweet, as measured by the number of ‘@’ mentions in the tweet string. A potential issue with this would be extraneous uses of the character ‘@’ or the mentioning of email addresses. We can assume that neither problem would occur much in the dataset. Tweets that mention multiple other users are perhaps less likely dealing with macroeconomic issues–maybe they’re related to personal interactions–so this predictor might be negatively related to market impact. exclamations: This variable measures the number of exclamation marks used in each tweet string. This could have predictive impact on the VIX because Trump’s use of emphasis on a tweet regarding, say, how China is “ripping off” America, might indicate to the market a greater leaning toward anti-China trade policies. links: This is the number of links in a Trump tweet, as measured by occurrences of the ‘http’ string. This may not have the most market importance, but it’s possible that the presence of links suggests that Trump is making some kind of comment with reference a particular news story or article, as opposed to making a pronouncement on a general issue of policy–implying reduced market impact. tweet_len: This is the length of a tweet. The logic underpinning the inclusion of this predictor is that longer tweets are likelier to be more substantive, and more substantive tweets are likelier to have an impact on the market. Exploratory Data Analysis We did an initial analysis of some keywords, as shown in our notebook. The list is ['trade', 'Trade', 'deal', 'products', 'manufacturing', 'China', 'Xi', 'Xi Jinping', 'CCP', 'Communist Party', 'Beijing']. Approximately 1/10 of Trump’s tweets that we cleaned (of ~14,000 total) contain some combination of these keywords. As we see above, the majority of the ‘keywords’ (based on our initial keyword list) appearing in our data are trade, Trade, deal, China, and Xi (products and manufacturing not far behind). CCP, Communist Party, and Beijing do not show up that frequently. In the future, we will probably employ functionality to search for the most frequently appearing useful words (excludes articles, punctuation, etc.). The histogram is reproduced below: The goal of looking at this list of keywords is to hone in on the content of Trump's tweets, specifically words and tweets in which we utilizes a negative tone toward China, and use tweet characteristics related to these keywords as relevant predictors in our model (such as how many of these keywords appear in a given tweet). Now, we look at the distribution of sentiment and subjectivity scores for our Twitter data. We see that most of the tweets, as classified by textblob, are mainly neutral and leaning positive if anything. Then, in terms of subjectivity, we see that the distribution is almost normal with a great chunk of tweets being objectively classified based on sentiment score. Below, we will create a sentiment score variable that is weighted by (1-subjectivity) to capture 'objective' sentiment score. Next, we examine the VIX data. Below are summary statistics for the data we were able to pull and consolidate from Bloomberg. Last Price	price_delta	price_delta_5	24_hr_perc	7_day_perc	14_day_perc	30_day_perc	52_week_high	52_week_perc	vix_delta_sign	year	month	hour	minutes count	722237.000000	722237.000000	722237.000000	721833.000000	719409.000000	716581.000000	710294.000000	676514.000000	676514.000000	722236.000000	722237.000000	722237.000000	722237.000000	722237.00000 mean	14.738460	-0.000022	-0.000087	0.001939	0.010396	0.016207	0.022988	38.205824	0.399478	0.000267	2017.472323	6.566354	9.413017	594.44071 std	4.103039	0.062353	0.125227	0.067065	0.163789	0.218262	0.275639	11.517582	0.103463	0.355108	1.113672	3.335537	3.796773	226.93971 min	8.910000	-12.690000	-12.730000	-0.430000	-0.560000	-0.610000	-0.610000	17.280000	0.210000	-1.000000	2015.000000	1.000000	3.000000	195.00000 25%	11.910000	-0.010000	-0.030000	-0.030000	-0.080000	-0.100000	-0.130000	26.720000	0.330000	0.000000	2017.000000	4.000000	6.000000	395.00000 50%	13.680000	0.000000	0.000000	0.000000	-0.010000	-0.020000	-0.020000	36.200000	0.400000	0.000000	2017.000000	7.000000	10.000000	610.00000 75%	16.580000	0.010000	0.030000	0.030000	0.070000	0.090000	0.110000	50.300000	0.450000	0.000000	2018.000000	9.000000	13.000000	791.00000 max	50.200000	8.570000	8.520000	1.490000	2.570000	3.320000	3.920000	53.290000	1.000000	1.000000	2019.000000	12.000000	16.000000	974.00000 Just based on the above, we can see a pretty large range in the values of the VIX, suggesting that the pricing jumps around a lot. At the same time, though, we see that on a minute by minute basis, the price change is very small. We keep this in mind when developing our models because even though are outcome is price delta, we experiment with different time intervals over which that delta is calculated to see which model will be most useful. Additionally, in our EDA, we looked at a snippet of the VIX pricing data. Our thought process was before diving into any analysis, we should first determine when the VIX rose or fell significantly and see if those changes appear to be related to any significant news events around those times. Our initial graph of the VIX data looks at EOD VIX pricing over a set period of time (December 1, 2015 - December 1, 2016). (Our analysis will hone in on minute by minute, but for us, it is important to be aware of the major surges). This graph is not reproduced below because of its size, but it can be seen in the notebook (title: “VIX Pricing over time”). The following were the main surges during this time period: 12/11/15 1/19/16 2/11/16 6/14/16 6/27/16 9/12/16 Two weeks before 2016 Presidential elections and a week afterwards For each date, we cross referenced it with the Financial Times Archives for the few days before and a few days after. Note: VIX is incredibly volatile, where for most metrics/indices a 3-4% change on any given day is regarded as a big move, such changes are the norm for VIX. Thus explaining any major surge due to one event remains challenging, yet for a high level understanding, we believe it to be necessary. Oil Prices Reaching Seven Year Lows Wall St. makes worst start to year, global bearishness, oil resumes slide, Fed might raise interest rates again Very mixed news, articles suggest renewed China risks due to China’s unruly peer-to-peer lending — 21 people arrested involved in “a complete Ponzi scheme” — ballooned in size last year as credit-starved private companies paid swingeing interest rates to secure loans Weak jobs data, major uncertainty regarding Fed hiking interest rates Post-Brexit jitters Oil price hikes, Merkel loses major state election in response to her open-door refugee policy Three major moves: first, as polling data shows narrowing of Clinton’s lead, the VIX continuously climbs, one of its more significant spikes being the day Fed reopened the investigation into her emails, secondly, one of VIX’s more significant downturns (though, still mild relative to the climbs of the previous weeks) was when the investigation was officially closed, and thirdly when Trump won, the VIX surged upwards, only to calm the day after when trading resumed, which was the exact trend following the Brexit vote. Note that the moves above are not directly related to Trump's tweets, but they did give us a sense for how striking news could affect VIX pricing on a minute by minute basis.",
    "url": "http://localhost:4000/final_notebook/data.html",
    "relUrl": "/final_notebook/data.html"
  },
  "3": {
    "id": "3",
    "title": "Home",
    "content": "Trump Tweet VIX (Group 3) TABLE OF CONTENTS Overview Website Navigation About Us Overview For our final project, we wanted to build a model that uses President Trump’s tweets to predict changes in the VIX Index. Theoretically, a model performing accurately 51 (or higher) percent of the time provides a way to intelligently bet on market volatility and generate a positive return on average. We utilized Trump’s Twitter archive here to build our database of tweets, and we pulled minute-by-minute VIX data from an HBS Baker Library Bloomberg Terminal. Our analysis begins in May 2016 when Trump emerged as a solid, leading candidate for the GOP in the 2016 Presidential election. insert info about models Source: CNN Website Navigation Our project background and question, as well as a brief literature review, can be found on the background page. Our data description and collection information, as well as our exploratory data analysis, can be found on the data exploration page. Our final models can be found on the models page. Our results, analysis, conclusions, and final discussion can be found on the conclusions page. About Us We are Gaurang Goel, Yashvardhan Bardoloi, and Adil Bhatia, Group 3 in CS109A, Fall 2019. Special thanks to Robbert Struyven and James Zeitler for their assistance.",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  },
  "4": {
    "id": "4",
    "title": "Models",
    "content": "Models TABLE OF CONTENTS Setup Baseline Model - Logistic L1 and L2 Regularization Random Forest Boosting Neural Networks Model Selection Setup Our goal for this project is to build a model that can accurately predict changes in VIX prices after Trump tweets. In the model, we use characteristics of the VIX data (like Last Price) and multiple characteristics of the Twitter data. Also, some predictors are common to both datasets, such as date/time predictors. Based on these predictors, we initially considered two types of models. First, we considered one that predicts absolute price changes in VIX pricing (continuous outcome). Second, we considered one that predicts the sign of VIX pricing changes (positive, negative, or no change - a categorical outcome). Intuitively, given what we know about the VIX and its pricing fluctuations, we thought it made more sense to focus on a model that predicts the change in pricing. Thus, we built a model that has a categorical output variable: -1 for negative price delta, 0 for no price delta, 1 for positive price delta. We also wanted to create interval versions of this model that looks at the VIX price change over 1 minute, 5, 10, 20 , 30, and 60 minutes. We wanted to consider all these options to determine the most accurate and most useful model. Realistically, if our model predicts well 10+ minutes after the tweet, it can offer a great chance to earn positive returns by trading on the VIX index. Another important consideration is the threshold at which we declare an outcome positive or negative. Given we are using minute-by-minute data, we decided upon a 0.001 threshold so that any change between -0.001 and 0.001 inclusive should be categorized as 0 (no price delta). To explain why we chose 0.001, there are a couple points to consider. First, any change is very helpful in terms of playing the market through trading, even if it is a small change. Second, a larger threshold could have potentially forced our models to disproportionately predict 0 change (at all time intervals), which would not be a useful model. As we will see below, at larger intervals, our models predict far fewer 0 outcomes, becoming more successful at predicting positive or negative changes in the VIX pricing over time. For each model trial, we first split our data into a train and test set, so that we are later able to assess how well our model performs in both a train set and a not-seen test set. Realistically, training accuracy reflects how well a given model ‘understands’ the data it is presented with, and testing accuracy reflects how well that model can be generalized to accurately form predictions about data it has not yet seen. For each time interval model, we drop the irrelevant time-based price predictors. For example, model30 does not include price_delta_30 or price_delta_60 as predictors. We perform this drop for each time interval model. Baseline Model - Logistic Our baseline model represents a simple logistic regression with a multiclass outcome variable. Using the predictors in our data, the model predicts the 'change in VIX price' classification as positive, negative, or 0 (no change) using a simple logistic regression. # 1 min logreg0 = LogisticRegression(C=10000).fit(X_train0, y_train0) logreg_fit_train0 = logreg0.predict(X_train0) logreg_fit_test0 = logreg0.predict(X_test0) train_scores_logreg0 = accuracy_score(y_train0, logreg_fit_train0) test_scores_logreg0 = accuracy_score(y_test0, logreg_fit_test0) print("Training Accuracy 1 min: ", train_scores_logreg0) print("Testing Accuracy 1 min: ", test_scores_logreg0) # 5 min logreg5 = LogisticRegression(C=10000).fit(X_train5, y_train5) logreg_fit_train5 = logreg5.predict(X_train5) logreg_fit_test5 = logreg5.predict(X_test5) train_scores_logreg5 = accuracy_score(y_train5, logreg_fit_train5) test_scores_logreg5 = accuracy_score(y_test5, logreg_fit_test5) print("Training Accuracy 5 min: ", train_scores_logreg5) print("Testing Accuracy 5 min: ", train_scores_logreg5) # 10 min logreg10 = LogisticRegression(C=10000).fit(X_train10, y_train10) logreg_fit_train10 = logreg10.predict(X_train10) logreg_fit_test10 = logreg10.predict(X_test10) train_scores_logreg10 = accuracy_score(y_train10, logreg_fit_train10) test_scores_logreg10 = accuracy_score(y_test10, logreg_fit_test10) print("Training Accuracy 10 min: ", train_scores_logreg10) print("Testing Accuracy 10 min: ", train_scores_logreg10) # 20 min logreg20 = LogisticRegression(C=10000).fit(X_train20, y_train20) logreg_fit_train20 = logreg20.predict(X_train20) logreg_fit_test20 = logreg20.predict(X_test20) train_scores_logreg20 = accuracy_score(y_train20, logreg_fit_train20) test_scores_logreg20 = accuracy_score(y_test20, logreg_fit_test20) print("Training Accuracy 20 min: ", train_scores_logreg20) print("Testing Accuracy 20 min: ", train_scores_logreg20) # 30 min logreg30 = LogisticRegression(C=10000).fit(X_train30, y_train30) logreg_fit_train30 = logreg30.predict(X_train30) logreg_fit_test30 = logreg30.predict(X_test30) train_scores_logreg30 = accuracy_score(y_train30, logreg_fit_train30) test_scores_logreg30 = accuracy_score(y_test30, logreg_fit_test30) print("Training Accuracy 30 min: ", train_scores_logreg30) print("Testing Accuracy 30 min: ", train_scores_logreg30) # 60 min logreg60 = LogisticRegression(C=10000).fit(X_train60, y_train60) logreg_fit_train60 = logreg60.predict(X_train60) logreg_fit_test60 = logreg60.predict(X_test60) train_scores_logreg60 = accuracy_score(y_train60, logreg_fit_train60) test_scores_logreg60 = accuracy_score(y_test60, logreg_fit_test60) print("Training Accuracy 60 min: ", train_scores_logreg60) print("Testing Accuracy 60 min: ", train_scores_logreg60) Interval  training accuracy   test accuracy 30 minute	0.523760	          0.503306 20 minute	0.515702	          0.500826 60 minute	0.534946	          0.499586 10 minute	0.491529	          0.476033 5 minute	0.476033	          0.452893 1 minute	0.468182	          0.451240 L1 and L2 Regularization We then decided to incorporate regularization in an attempt to improve our logistic model's predictive ability. Lasso regularization (L1) sets the effects/coefficients of unimportant predictors to 0, whereas ridge (L2) simply minimizes/lowers those effects. First, lasso regularization: from sklearn.linear_model import LogisticRegressionCV # lasso lasso = LogisticRegressionCV(cv=5, penalty='l1', max_iter=1000, solver='liblinear') train_scores_logreg_lasso = [] test_scores_logreg_lasso = [] for i in range(len(X_train_list)): lassofit = lasso.fit(X_train_list[i], y_train_list[i]) y_pred_train_lasso = lassofit.predict(X_train_list[i]) y_pred_test_lasso = lassofit.predict(X_test_list[i]) train_score = accuracy_score(y_train_list[i], y_pred_train_lasso) test_score = accuracy_score(y_test_list[i], y_pred_test_lasso) train_scores_logreg_lasso.append(train_score) test_scores_logreg_lasso.append(test_score) print(f'Training set accuracy score for {intervals[i]} using CV & LASSO penalty: {train_score:.4f}') print(f'Test set accuracy score for {intervals[i]} using CV & LASSO penalty: {test_score:.4f}') Interval	training accuracy	test accuracy 1 minute	0.467769	          0.457025 5 minute	0.457025	          0.455372 60 minute	0.444582	          0.419355 30 minute	0.439669	          0.416529 20 minute	0.373140	          0.348760 10 minute	0.217149	          0.200826 Now, ridge regularization: # ridge ridge = LogisticRegressionCV(cv=5, penalty='l2', max_iter=1000, solver='liblinear') train_scores_logreg_ridge = [] test_scores_logreg_ridge = [] for i in range(len(X_train_list)): ridgefit = ridge.fit(X_train_list[i], y_train_list[i]) y_pred_train_ridge = ridgefit.predict(X_train_list[i]) y_pred_test_ridge = ridgefit.predict(X_test_list[i]) train_score = accuracy_score(y_train_list[i], y_pred_train_ridge) test_score = accuracy_score(y_test_list[i], y_pred_test_ridge) train_scores_logreg_ridge.append(train_score) test_scores_logreg_ridge.append(test_score) print(f'Training set accuracy score for {intervals[i]} using CV & Ridge penalty:: {train_score:.4f}') print(f'Test set accuracy score for {intervals[i]} using CV & Ridge penalty:: {test_score:.4f}') Interval	training accuracy	test accuracy 60 minute	0.544458	          0.516129 20 minute	0.526033	          0.500826 30 minute	0.531612	          0.495868 10 minute	0.495455	          0.480165 1 minute	0.469421	          0.451240 5 minute	0.473140	          0.439669 Lasso Regularization does not perform well, whereas ridge gets us just above 50% accuracy on the test set. This suggests that a few predictors may have significant impact and are being pushed to 0 improperly in lasso. Random Forest Our first ensemble method is random forest, which randomly subsets predictors upon which to generate decision trees. We tested out a few different tree depth and number parameters ourselves and determined that a depth of 5 and number of trees of 100 was ideal for our analysis. Below is sample code for one of the time interval models. We perform this for each of the models, and the results can bee seen below this code. # Calibrate num trees and tree depth (after having tried different parameters, saw comparable results) n_trees = 100 tree_depth = 5 # create RF models forest_model0 = RandomForestClassifier(n_estimators=n_trees, max_depth=tree_depth, max_features=int(np.sqrt(len(np_X_train0[0])))) forest_model5 = RandomForestClassifier(n_estimators=n_trees, max_depth=tree_depth, max_features=int(np.sqrt(len(np_X_train5[0])))) forest_model10 = RandomForestClassifier(n_estimators=n_trees, max_depth=tree_depth, max_features=int(np.sqrt(len(np_X_train10[0])))) forest_model20 = RandomForestClassifier(n_estimators=n_trees, max_depth=tree_depth, max_features=int(np.sqrt(len(np_X_train20[0])))) forest_model30 = RandomForestClassifier(n_estimators=n_trees, max_depth=tree_depth, max_features=int(np.sqrt(len(np_X_train30[0])))) forest_model60 = RandomForestClassifier(n_estimators=n_trees, max_depth=tree_depth, max_features=int(np.sqrt(len(np_X_train60[0])))) # fit and calculate accuracy forest_model0.fit(np_X_train0, np_y_train0) y_pred0_forest_train = forest_model0.predict(np_X_train0) y_pred0_forest_test = forest_model0.predict(np_X_test0) random_forest_train_score0 = accuracy_score(np_y_train0, y_pred0_forest_train) random_forest_test_score0 = accuracy_score(np_y_test0, y_pred0_forest_test) print(f'The random forest accuracy on the training set: {random_forest_train_score0}') print(f'The random forest accuracy on the test set: {random_forest_test_score0:.4f}') Interval	training accuracy	test accuracy 60 minute	0.620968	          0.545906 20 minute	0.623347	          0.532231 30 minute	0.611777	          0.514050 10 minute	0.635331	          0.508264 1 minute	0.547934	          0.455372 5 minute	0.591116	          0.446281 Random Forest performs decently relative to our original goal in the project (achieving above 50% test accuracy). Boosting Next, we will consider boosting, an iterative approach that might eliminate some more of the error in our trees. Below is sample code for one of the time interval models. We perform this for each of the models, and the results can bee seen below this code. # initialize parameters like for RF, much the same process - tested different ones and saw best/most consistent results with the following estimators_ADA = 40 learning_ADA = 0.01 tree_depth = 10 model_ADA0 = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=tree_depth), learning_rate=learning_ADA, n_estimators=estimators_ADA) model_ADA0.fit(np_X_train0, np_y_train0) y_train_pred_ADA0 = model_ADA0.predict(np_X_train0) y_test_pred_ADA0 = model_ADA0.predict(np_X_test0) ADA_train0 = accuracy_score(np_y_train0, y_train_pred_ADA0) ADA_test0 = accuracy_score(np_y_test0, y_test_pred_ADA0) print(f'The ADABoost accuracy on the training set: {ADA_train0}') print(f'The ADABoost accuracy on the test set: {ADA_test0:.4f}') ADA_train0_staged = list(model_ADA0.staged_score(np_X_train0, np_y_train0)) ADA_test0_staged = list(model_ADA0.staged_score(np_X_test0, np_y_test0)) # define function to abstract process of building plot def baselearner_plt(n, X_train, y_train, X_test, y_test): # AdaBoostClassifier ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=n), n_estimators=30, learning_rate = 0.05) ada.fit(X_train, y_train) # staged_score train to plot ada_predicts_train = list(ada.staged_score(X_train,y_train)) plt.plot(ada_predicts_train, label = "Train"); # staged_score test to plot ada_predicts_test = list(ada.staged_score(X_test,y_test)) plt.plot(ada_predicts_test, label = "Test"); plt.legend(fontsize=14) plt.title("AdaBoost Classifier Accuracy", fontsize=18) plt.ylabel("Accuracy", fontsize=16) plt.xlabel("Num Iterations", fontsize=16) plt.show() print("Maximum test accuracy for depth of "+str(n)+" is "+str(max(ada_predicts_test))+" at "+str(ada_predicts_test.index(max(ada_predicts_test)))+" iterations") for i in range(len(X_train_list)): baselearner_plt(tree_depth, X_train_list[i], y_train_list[i], X_test_list[i], y_test_list[i]) print("Time Interval: ", intervals[i]) png Maximum test accuracy for depth of 10 is 0.4818181818181818 at 2 iterations Time Interval:  1 minute png Maximum test accuracy for depth of 10 is 0.5107438016528926 at 8 iterations Time Interval:  5 minute png Maximum test accuracy for depth of 10 is 0.5958677685950413 at 9 iterations Time Interval:  10 minute png Maximum test accuracy for depth of 10 is 0.6090909090909091 at 21 iterations Time Interval:  20 minute png Maximum test accuracy for depth of 10 is 0.6347107438016529 at 29 iterations Time Interval:  30 minute png Maximum test accuracy for depth of 10 is 0.6683209263854425 at 5 iterations Time Interval:  60 minute Boosting performs relatively well when comparing all the models. Let’s try a NN to see if we can do better, though. Neural Networks Finally, we created an artificial neural network to classify the changes in VIX price. In this model, we shrunk the number of categories to 2, positive and negative, to see how well this baseline NN performs. # prepare model model = models.Sequential() for i in range(5): model.add(tf.keras.layers.Dense(100, activation='relu')) model.add(layers.Dropout(0.3)) model.add(tf.keras.layers.Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # define parameters epochs = 100 batch_size = 12 validation_split = 0.3 # fit model history0 = model.fit(np_X_train0_NN, np_y_train0_NN, validation_split=validation_split, epochs=epochs, batch_size=batch_size, verbose=False) history5 = model.fit(np_X_train5_NN, np_y_train5_NN, validation_split=validation_split, epochs=epochs, batch_size=batch_size, verbose=False) history10 = model.fit(np_X_train10_NN, np_y_train10_NN, validation_split=validation_split, epochs=epochs, batch_size=batch_size, verbose=False) history20 = model.fit(np_X_train20_NN, np_y_train20_NN, validation_split=validation_split, epochs=epochs, batch_size=batch_size, verbose=False) history30 = model.fit(np_X_train30_NN, np_y_train30_NN, validation_split=validation_split, epochs=epochs, batch_size=batch_size, verbose=False) history60 = model.fit(np_X_train60_NN, np_y_train60_NN, validation_split=validation_split, epochs=epochs, batch_size=batch_size, verbose=False) # output relevant information histories = [history0, history5, history10, history20, history30, history60] for i, history in zip(range(len(intervals)), histories): kaggle_train_acc = history.history['accuracy'][-1] val_loss = history.history['val_loss'][-1] val_acc = history.history['val_accuracy'][-1] diff = val_acc - val_loss loss = history.history['loss'][-1] model.summary() print(f'{intervals[i]} \n ModelTraining Accuracy={kaggle_train_acc}, \n Training Loss={loss}, \n Model Validation Accuracy: {val_acc}, \n Model Validation Loss: {val_loss}, \n Difference between Validation Accuracy and Loss: {diff} \n') Interval	training accuracy	val accuracy	val loss 60 minute	0.844609	          0.618194	          0.369464 30 minute	0.843566	          0.606749	          0.376075 20 minute	0.826741	          0.583333	          0.402431 1 minute	0.789256	          0.573691	          0.426097 10 minute	0.817001	          0.570248	          0.413901 5 minute	0.784829	          0.554408	          0.442316 Even after changing hyperparameters, our neural network does performs pretty much in line with our other models and slightly below boosting. Model Selection Based upon the presented analysis, we conclude that our boosted decision tree classifier at the 60 min time interval, at a depth of 10 with 5 iterations, is the best model. It achieves the highest accuracy in the test set, of 66.8%.",
    "url": "http://localhost:4000/final_notebook/models.html",
    "relUrl": "/final_notebook/models.html"
  }
}

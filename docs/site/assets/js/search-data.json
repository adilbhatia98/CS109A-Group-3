{
  "0": {
    "id": "0",
    "title": "Background",
    "content": "Background  Project Question  Project Motivation Approach Literature Review How do President Trump’s tweets impact the volatility of the S&P 500? How has that impact changed over time, if at all, since he began his term as U.S. President? In September 2019, J.P. Morgan launched their ‘Volfefe’ index meant to track U.S. treasury bond volatility based on market sentiment changes due to unpredictable tweets from President Trump. Using a portion of Trump’s tweets since 2017, JPM developed a model that predicted bond market volatility surprisingly accurately. In our project, we examine VIX, the first benchmark index to measure expectations of future market volatility. VIX prices are based on S&P 500 options, and rapid and/or significant price changes usually reflect sudden changes in market sentiment. For example, if VIX jumps 10% in a given day, it suggests that investors are generally more unsure about the future of the U.S. economy, resulting in greater volatility in the overall market. We had three goals with this project: create a well-performing model that can predict changes in VIX based on Trump’s tweets use that model to generate positive returns in the coming future produce a clean interface using jekyll to display our project analysis and results so that others can replicate our method and hopefully improve it! Approach UPDATE We first downloaded Trump’s twitter database and cleaned the data to focus on the tweet content. Then, we analyzed it using textblob to generate a sentiment score for each tweet. Next, we went to HBS and manually pulled minute-by-minute VIX data (a cumbersome process, but one we believe was worth the effort!). Then, we consolidated this data into a single dataframe. We then randomly split this data into a training and test set with the outcome variable being the change in VIX pricing and the core explanatory variable being the sentiment score for a given tweet, along with several other predictors constructed based on the twitter and VIX data. Predicting the absolute VIX price yielded irrelevant results (nonsurprisingly), but predicting the change in VIX price produced some interesting results. Next, we built and fit a variety of classifiers with differing VIX price change time intervals. Models we built include: Baseline Model Logistic Classifier For each model, we evaluated its accuracy on both our training and and test set. Based on accuracy scores, we determined the classifier with the highest performance on the test set. Finally, we ran our best-performing model on a fresh set of songs and asked Grace if she liked her new playlist. Literature Review Using Twitter to Predict the Stock Market Attemt to extract mood levels from Social Media applications in order to predict stock returns Uses roughly 100 million tweets that were published in Germany between January, 2011 and November, 2013 Find that it is necessary to take into account the spread of mood states among Internet users Portfolio increases by up to 36 % within a sixmonth period after the consideration of transaction costs Data Mining Twitter To Predict Stock Market Movements Sentiment analysis of Twitter data from July through December, 2013 Attempt to find correlation between users’ sentiments and NASDAQ closing price and trading volume Find that “Happy” and “sad” sentiment variables’ lags are strongly correlated with closing price and “excited” and “calm” lags are strongly correlated with trading volume Incorporates interesting word weighting to classify tweets into emotional groupings A topic-based sentiment analysis model to predict stock market price movement using Weibo mood Demonstrate that the emotions automatically extracted from the large scale Weibo posts represent the real public opinions about some special topics of the stock market in China. Nonlinear autoregressive model, using neural network with exogenous sentiment inputs is proposed to predict the stock price movement. “Given the performance, if more related topics for a stock are found out, the accuracy could be higher considering more completed topic-based sentiment inputs.” Recognize that the time lag on sentiment makes it difficult to fully predict market changes Can Twitter Help Predict Firm-Level Earnings and Stock Returns? Test whether opinions of individuals tweeted just prior to a firm’s earnings announcement predict its earnings and announcement returns Find that the aggregate opinion from individual tweets successfully predicts a firm’s forthcoming quarterly earnings and announcement returns",
    "url": "http://localhost:4000/background.html",
    "relUrl": "/background.html"
  },
  "1": {
    "id": "1",
    "title": "Conclusions",
    "content": "Conclusions TABLE OF CONTENTS Analysis of Results Extending Our Model Limitations and Future Work Analysis of Results Our best model is the boosted decision tree classifier with a depth of 2 and 751 iterations, which performs with an accuracy of 95.4% in the training set and 93.0% in the test set. The following table summarizes the accuracies for all our models, ordered by accuracy in the test set: Model Type	Train Accuracy	Test Accuracy Naive Model	50.3%	51.6% Neural Network	62.8%	65.2% kNN	63.1%	65.9% Logistic Regression With L2 Regularization	69.2%	66.9% Baseline Logistic Regression	69.4%	67.1% QDA	86.6%	86.7% LDA	88.1%	88.4% Logistic Regression With L1 Regularization	88.6%	88.7% Decision Tree Classifier	88.0%	89.0% Decision Tree Classifier With Bagging	93.6%	91.8% Random Forest	92.9%	92.0% Boosted Decision Tree Classifier	95.4%	93.0% Our lowest performing models include the logistic regression with the naive model, the neural network, and the kNN model. Our best performing models were all ensemble methods. The boosted decision tree classifier, the random forest model, and the decision tree classifier with bagging performed best. We tuned the parameters and hyperparameters of each base model to maximize the accuracy score of each, which leads us to believe that we achieved the maximum possible classification accuracy given the constraints of our dataset. Indeed, this model performs much better than the baseline model, which achieves an accuracy of only 51.6% in the test set. Finally, while usually time and space are considerations when evaluating different types of models, because they do not constrain our original problem, we chose to focus on accuracy. However, a qualitative assessment of these metrics determined that all models were comparable in terms of runtime and memory use with the exception of the neural nets that took additional time. One consideration to note is that the boosted model is less suited for parallelization than other ensemble methods, because it is iterative. However, the runtime was less than a few seconds, so we will prioritize the accuracy of the model over this concern. — Extending Our Model We can now try to generate a playlist customized to Grace’s taste using our chosen model. We will present the model with a list of songs that both Grace and the model have not seen before. We’ll then have the model assess whether these songs should be included in the playlist and then verify that with Grace’s opinion. # load in dataset full_songs_df = pd.read_csv("data/spotify-test.csv") # drop unnecessary columns songs_df = full_songs_df.drop(columns=['type', 'id', 'uri', 'track_href', 'analysis_url', 'name', 'artist', 'Unnamed: 0']) # recreating the best model best_abc = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=800, learning_rate = 0.05) best_abc.fit(x_train, y_train) predictions = best_abc.predict(songs_df) print("Songs Selected for Grace's Playlist") for i in range(len(predictions)): if predictions[i] == 1: print(full_songs_df.loc[i]['name']) Songs Selected for Grace's Playlist Never Seen Anything "Quite Like You" - Acoustic Crazy World - Live from Dublin Whatever You Do Come on Love 1,000 Years Machine After Dark Sudden Love (Acoustic) Georgia I Don't Know About You This randomly selected dataset had 26 songs. These songs had never been classified by Grace before and our best model (the boosted decision tree classifier with a depth of 2) was used to predict whether songs would be included in her playlist. We then played all the songs in the dataset to Grace to see whether she would include them in her playlist. The model performed accurately, except for one song which she said she would not have added to her playlist (“I Don’t Know About You”). One reason for this mishap could be that our model isn’t 100% accurate, so this song could be by chance one of the ones it messes up; 1 missed song out of 26 is reasonable for a model with 93% accuracy. Another reason could be that Grace’s actual taste is different from how she made the playlist (perhaps she is in a different emotive or environmental state that temporally affects her preferences, or perhaps her underlying preferences have changed). Despite this error, overall, Grace was pleased that we could use data science to automate her playlist selection procees! Limitations and Future Work Data Size We generated a dataset by consolidating a large array of songs that vary in genre, language, tempo, rhythm, etc. We tried to curate a dataset that mimicked the variety of songs that Spotify has. Grace then had to go through these songs and classify whether she would like them in her playlist or not. Due to a multitude of constraints, we only had 5000 songs between both and training and test data. Ideally more songs that accurately capture the variety of songs that Spotify has would improve the training procedures for models. Data Inclusion Outside of the side of the data set, there are other data sets that can be used discover new predictors or variables about songs. We can explore lyrics for example and see how that contributes to a model’s recommendations. Thus requires us expanding beyond the SpotifyAPI and exploring other data sets. Adapting Playlists This entire project was built off of the preferences of one individual: Grace. While this proved to be a good proof of concept, future exploration should be done to analyze how the best model can help create playlists for others based upon their interests. Collaborative Filtering Collaborative filtering is another type of data modeling that is commonly used for recommendation algorithms. It is based on the fundamental idea that people perfer things similar to the things they’ve established they like. As such, it would be a good model to further investigate for this given project. Improve Neural Network Our neural network did not perform particularly well. While we tuned many hyperparameters, further tuning, exploring other network structures, and changing optimizers may help improve our network. Additionally, we could consider using convolutional neural networks. Dynamic Preferences Finally, a review of the literature highlighted the importance of dynamic preferences. Individuals often adapt the music they want to listen to at a particular time based on their emotions or external situation. Allowing for a modification of models to include these parameters could be useful. For example, if Grace tracked the time of day that she added each song to her playlist and we could use that time as a feature, we could better suggest songs suited to a particular time of day. This could help adjust for temporal effects, such as desiring certain music during a morning run or commute to work, versus desiring different music for an evening shower, versus desiring different music for a party late at night.",
    "url": "http://localhost:4000/conclusions.html",
    "relUrl": "/conclusions.html"
  },
  "2": {
    "id": "2",
    "title": "Data Exploration",
    "content": "Data Exploration TABLE OF CONTENTS Data Collection and Cleaning Twitter VIX Consolidated Data Data Description Exploratory Data Analysis Data Collection and Cleaning Twitter We pulled all of Trump’s tweets in the last few years from his Twitter archive. Due to the naature of the database, we spent much time cleaning this data for our purposes. We took the following steps to clean the data:</p> Importing raw data from the archive Removing unnecessary columns Adjusting GMT to Eastern Time + accounting for daylight savings Manually fixing errors in cells where the delimiting was incorrectly done in the database output and manually re-inserting the delimiting character We utilized ntlk</code’s textblob function in order to analyze the sentiment of tweets in our data set. For each tweet, this function created a polarity score (the more positive a tweet is, the closer the score is to 1; the more negative, the closer it is to -1). The function also returns a subjectivity score. Lower subjectivity score means that the tweet’s polarity score more objectively represents its sentiment. VIX VIX is the first benchmark index to measure expectations of future market volatility (based on S&P 500 options). Since Milestone 2, we have procured minute-by-minute VIX data from 12/2015 - 11/11/2019. This data includes only VIX pricing on trading days throughout the past few years (excludes holidays, weekends, etc.). We manually pulled the data from a Bloomberg Terminal at HBS Baker Library. Given the size of the dataset and the Terminal download limits, we manually copied and pasted the VIX data directly into a csv file. Note that all other sources of VIX data are at best day-by-day and typically cost a nontrivial amount. VIX is managed by CBOE (Chicago Board Options Exchange). The global trading hours for VIX can be found here. Trading hours range from 3:15 am EST until 4:15 pm EST. There is a break bewteen 9:15 and 9:30 am, but this is addressed in how we consolidate our data. Consolidated Data In order to consolidate the data, we merge our Twitter and VIX dataframes on date/time to ensure that we are only looking at tweets that are posted during VIX trading hours. That way, we do not have to worry about tweets occurring outside these hours. Also, when we look at the change in VIX price, we are only looking at changes during trading hours, so examining only tweets that are posted during trading hours allows us to perform this analysis soundly. from textblob import TextBlob tweets = twitter_archive_df['text'] tweets_list = [tweet for tweet in tweets] big_tweet_string = ' '.join(tweets_list) tokens = word_tokenize(big_tweet_string) words = [word.lower() for word in tokens if word.isalpha()] stop_words = set(stopwords.words('english')) words = [word for word in words if not word in stop_words] scores = [] subjectivity = [] for tweet in tweets_list: blob_tweet = TextBlob(tweet) sentiment = blob_tweet.sentiment score = sentiment[0] subject = sentiment[1] scores.append(float(score)) subjectivity.append(float(subject)) twitter_archive_df['sent_score'] = scores twitter_archive_df['subjectivity'] = subjectivity # twitter_archive_df.head() Data Description Our data includes the following features: danceability: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable. energy: Energy represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy. A value of 0.0 is least energetic and 1.0 is most energetic. key: The estimated overall key of the track. Integers map to pitches using standard Pitch Class Notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1. loudness: The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values range between -60 and 0 db. mode: Mode represents the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Mode is binary; major is represented by 1 and minor is 0. speechiness: Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks. acousticness: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic. instrumentalness: Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0. liveness: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live. valence: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry). tempo: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration. duration_ms: The duration of the track in milliseconds. time_signature: An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). popularity: The popularity of a track is a value between 0 and 100, with 100 being the most popular. The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are.Generally speaking, songs that are being played a lot now will have a higher popularity than songs that were played a lot in the past. in_playlist: Response variable. Categorical variable for whether in playlist of desire. 1 if in playlist, 0 if not in playlist. The following features were recorded to help with visualization later, but not used as predictors in our analysis, as they are not characteristics of the music itself. name: Song title artist: First artist of song type: The object type, always deemed ‘audio_features.’ id: The Spotify ID for the track. uri: The Spotify URI for the track. track_href: A link to the Web API endpoint providing full details of the track. analysis_url: An HTTP URL to access the full audio analysis of this track. An access token is required to access this data. Exploratory Data Analysis We did an initial analysis of some keywords, as shown in our notebook. The list is ['trade', 'Trade', 'deal', 'products', 'manufacturing', 'China', 'Xi', 'Xi Jinping', 'CCP', 'Communist Party', 'Beijing']. Approximately 1/10 of Trump’s tweets that we cleaned (of ~14,000 total) contain some combination of these keywords. As we see above, the majority of the ‘keywords’ (based on our initial keyword list) appearing in our data are trade, Trade, deal, China, and Xi (products and manufacturing not far behind). CCP, Communist Party, and Beijing do not show up that frequently. In the future, we will probably employ functionality to search for the most frequently appearing useful words (excludes articles, punctuation, etc.). The histogram is reproduced below: Next, we examine the VIX data. Below are summary statistics for the data we were able to pull and consolidate from Bloomberg. Last Price	price_delta	price_delta_5	24_hr_perc	7_day_perc	14_day_perc	30_day_perc	52_week_high	52_week_perc	vix_delta_sign	year	month	hour	minutes count	722237.000000	722237.000000	722237.000000	721833.000000	719409.000000	716581.000000	710294.000000	676514.000000	676514.000000	722236.000000	722237.000000	722237.000000	722237.000000	722237.00000 mean	14.738460	-0.000022	-0.000087	0.001939	0.010396	0.016207	0.022988	38.205824	0.399478	0.000267	2017.472323	6.566354	9.413017	594.44071 std	4.103039	0.062353	0.125227	0.067065	0.163789	0.218262	0.275639	11.517582	0.103463	0.355108	1.113672	3.335537	3.796773	226.93971 min	8.910000	-12.690000	-12.730000	-0.430000	-0.560000	-0.610000	-0.610000	17.280000	0.210000	-1.000000	2015.000000	1.000000	3.000000	195.00000 25%	11.910000	-0.010000	-0.030000	-0.030000	-0.080000	-0.100000	-0.130000	26.720000	0.330000	0.000000	2017.000000	4.000000	6.000000	395.00000 50%	13.680000	0.000000	0.000000	0.000000	-0.010000	-0.020000	-0.020000	36.200000	0.400000	0.000000	2017.000000	7.000000	10.000000	610.00000 75%	16.580000	0.010000	0.030000	0.030000	0.070000	0.090000	0.110000	50.300000	0.450000	0.000000	2018.000000	9.000000	13.000000	791.00000 max	50.200000	8.570000	8.520000	1.490000	2.570000	3.320000	3.920000	53.290000	1.000000	1.000000	2019.000000	12.000000	16.000000	974.00000 Just based on the above, we can see a pretty large range in the values of the VIX, suggesting that the pricing jumps around a lot. At the same time, though, we see that on a minute by minute basis, the price change is very small. We keep this in mind when developing our models because even though are outcome is price delta, we experiment with different time intervals over which that delta is calculated to see which model will be most useful. Additionally, in our EDA, we looked at a snippet of the VIX pricing data. Our thought process was before diving into any analysis, we should first determine when the VIX rose or fell significantly and see if those changes appear to be related to any significant news events around those times. Our initial graph of the VIX data looks at EOD VIX pricing over a set period of time (December 1, 2015 - December 1, 2016). (Our analysis will hone in on minute by minute, but for us, it is important to be aware of the major surges). This graph is not reproduced below because of its size, but it can be seen in the notebook (title: “VIX Pricing over time”). The following were the main surges during this time period: 12/11/15 1/19/16 2/11/16 6/14/16 6/27/16 9/12/16 Two weeks before 2016 Presidential elections and a week afterwards For each date, we cross referenced it with the Financial Times Archives for the few days before and a few days after. Note: VIX is incredibly volatile, where for most metrics/indices a 3-4% change on any given day is regarded as a big move, such changes are the norm for VIX. Thus explaining any major surge due to one event remains challenging, yet for a high level understanding, we believe it to be necessary. Oil Prices Reaching Seven Year Lows Wall St. makes worst start to year, global bearishness, oil resumes slide, Fed might raise interest rates again Very mixed news, articles suggest renewed China risks due to China’s unruly peer-to-peer lending — 21 people arrested involved in “a complete Ponzi scheme” — ballooned in size last year as credit-starved private companies paid swingeing interest rates to secure loans Weak jobs data, major uncertainty regarding Fed hiking interest rates Post-Brexit jitters Oil price hikes, Merkel loses major state election in response to her open-door refugee policy Three major moves: first, as polling data shows narrowing of Clinton’s lead, the VIX continuously climbs, one of its more significant spikes being the day Fed reopened the investigation into her emails, secondly, one of VIX’s more significant downturns (though, still mild relative to the climbs of the previous weeks) was when the investigation was officially closed, and thirdly when Trump won, the VIX surged upwards, only to calm the day after when trading resumed, which was the exact trend following the Brexit vote. Note that the moves above are not directly related to Trump's tweets, but they did give us a sense for how striking news could affect VIX pricing on a minute by minute basis.",
    "url": "http://localhost:4000/final_notebook/data.html",
    "relUrl": "/final_notebook/data.html"
  },
  "3": {
    "id": "3",
    "title": "Home",
    "content": "Trump Tweet VIX (Group 3) TABLE OF CONTENTS Overview Website Navigation About Us Overview For our final project, we wanted to build a model that uses President Trump’s tweets to predict changes in the VIX Index. Theoretically, a model performing accurately 51 (or higher) percent of the time provides a way to intelligently bet on market volatility and generate a positive return on average. We utilized Trump’s Twitter archive here to build our database of tweets, and we pulled minute-by-minute VIX data from an HBS Baker Library Bloomberg Terminal. Our analysis begins in May 2016 when Trump emerged as a solid, leading candidate for the GOP in the 2016 Presidential election. insert info about models Source: CNN Website Navigation Our project background and question, as well as a brief literature review, can be found on the background page. Our data description and collection information, as well as our exploratory data analysis, can be found on the data exploration page. Our final models can be found on the models page. Our results, analysis, conclusions, and final discussion can be found on the conclusions page. About Us We are Gaurang Goel, Yashvardhan Bardoloi, and Adil Bhatia, Group 3 in CS109A, Fall 2019. Special thanks to Robbert Struyven and James Zeitler for their assistance.",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  },
  "4": {
    "id": "4",
    "title": "Models",
    "content": "Models  Setup  Baseline Model  Logistic Classifier  Logistic Classifier with Quadratic Terms  L1 and L2 Regularization  kNN  LDA and QDA  Decision Trees  Bagging  Random Forest  Boosting  Neural Networks  Model SelectionOur goal for this project is to construct a list of songs from a new list of songs that Grace would like to add to her existing playlist. We attempt to do this by training a variety of different models to predict whether or not a song should be included in Grace’s playlist. By framing the question in this way, we recognize that playlist construction can be considered from a classification perspective, where we need to classify each song in our test set as either being in_playlist or not in_playlist.SetupWe first split our data into a train and test set, so that we are later able to assess how well our model performs in both a train set and a not-seen test set.train, test = train_test_split(spotify_df, test_size = 0.2, random_state=50)x_train, y_train = train.drop(columns=[response_col]), train[response_col].valuesx_test, y_test = test.drop(columns=[response_col]), test[response_col].valuesBaseline ModelWe began our project by first constructing a baseline model - one where we simply predict that all songs should be included in our existing playlist. This serves as a good source of comparison for our future models, which should at least do better than this trivial one.baseline_train_score = np.sum(y_train == 1) / len(train)baseline_test_score = np.sum(y_test == 1) / len(test)print('Baseline model (all songs are added to the existing playlist) train score:', baseline_train_score)print('Baseline model (all songs are added to the existing playlist) test score:', baseline_test_score)Baseline model (all songs are added to the existing playlist) train score: 0.5034584980237155Baseline model (all songs are added to the existing playlist) test score: 0.5158102766798419We can see that our trivial model does not perform very well in either the train or test set, achieving 50.3% accuracy in the train set and 51.6% accuracy in the test set.Logistic ClassifierGiven our objective, we next considered simple, interpretable models that could help us classify our data. A clear option was the logistic model, which works well for binary classifications.We fit a logistic classifier on all 14 features of our training data.# set seedrandom.seed(1)# split into train and testtrain, test = train_test_split(spotify_df, test_size = 0.2, random_state=50)x_train, y_train = train.drop(columns=[response_col]), train[response_col].valuesx_test, y_test = test.drop(columns=[response_col]), test[response_col].values# create logistic modellog_reg_model = LogisticRegression(C=100000, fit_intercept=False)log_reg_model.fit(x_train, y_train)# predictlog_reg_train_predictions = log_reg_model.predict(x_train)log_reg_test_predictions = log_reg_model.predict(x_test)# calculate scoreslog_reg_train_score = accuracy_score(y_train, log_reg_train_predictions)log_reg_test_score = accuracy_score(y_test, log_reg_test_predictions)# display scoresprint('[Logistic Regression] Classification accuracy for train set: {}'.format(log_reg_train_score))print('[Logistic Regression] Classification accuracy for test set: {}'.format(log_reg_test_score))[Logistic Regression] Classification accuracy for train set: 0.6936758893280632[Logistic Regression] Classification accuracy for test set: 0.6709486166007905Our baseline logistic model is able to achieve an accuracy of roughly 69.4% in the training set, and 67.1% in the test set. We can see that this is already better than our trivial baseline model, which is a great sign! However, we believe we can build an even better predictive model.Logistic Classifier with Quadratic TermsOur next appraoch was to consider a logistic regression model that includes quadratic terms as well as main effect terms, in an attempt to capture any polynomial relationships that may exist between our features and whether or not a particular song should be included in our playlist.# add quadratic termsx_train_q = x_train.copy()x_test_q = x_test.copy()# add quadratic termsfor col in x_train:    if col != &quot;mode&quot;: # our only binary variable        name = col + &quot;^2&quot; # name column as col^2        x_train_q[name] = np.square(x_train_q[col])        x_test_q[name] = np.square(x_test_q[col])# create logistic modellog_reg_model_q = LogisticRegression(C=100000, fit_intercept=False)log_reg_model_q.fit(x_train_q, y_train)# predictlog_reg_train_q_predictions = log_reg_model_q.predict(x_train_q)log_reg_test_q_predictions = log_reg_model_q.predict(x_test_q)# calculate scoreslog_reg_train_q_score = accuracy_score(y_train, log_reg_train_q_predictions)log_reg_test_q_score = accuracy_score(y_test, log_reg_test_q_predictions)# display scoresprint('[Logistic Regression With Quadratic Terms] Classification accuracy for train set: {}'.format(log_reg_train_q_score))print('[Logistic Regression With Quadratic Terms] Classification accuracy for test set: {}'.format(log_reg_test_q_score))[Logistic Regression With Quadratic Terms] Classification accuracy for train set: 0.4965415019762846[Logistic Regression With Quadratic Terms] Classification accuracy for test set: 0.4841897233201581However, after adding quadratic terms to our model, we see that the model performs worse. The test and training accuracies are both quite low at roughly 48.4% and 49.7%.L1 and L2 RegularizationGiven our low scores on our logistic regression model with quadratic terms, we consider adding regularization to our model to make sure that we are not overfitting to our training data. We consider both L1 and L2 regularization.# L1 regularizationlr_l1_model = LogisticRegressionCV(cv=5, penalty='l1', solver='liblinear', max_iter=100000).fit(x_train, y_train)# L2 regularizationlr_l2_model = LogisticRegressionCV(cv=5, max_iter=100000).fit(x_train, y_train)def get_lr_cv(model, model_name, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test):    train_predictions = model.predict(x_train)    train_score = accuracy_score(y_train, train_predictions)    test_predictions = model.predict(x_test)    test_score = accuracy_score(y_test, test_predictions)    test_confusion_matrix = confusion_matrix(y_test, test_predictions)    print('[{}] Classification accuracy for train set: {}'.format(model_name, train_score))    print('[{}] Classification accuracy for test set: {}'.format(model_name, test_score))    return train_score, test_score, test_confusion_matrixl1_stats = get_lr_cv(lr_l1_model, 'L1 Reg')l2_stats = get_lr_cv(lr_l2_model, 'L2 Reg')[L1 Reg] Classification accuracy for train set: 0.8866106719367589[L1 Reg] Classification accuracy for test set: 0.8873517786561265[L2 Reg] Classification accuracy for train set: 0.6926877470355731[L2 Reg] Classification accuracy for test set: 0.6699604743083004We can see that L1 regularization performs much better than L2. The L1 regularized model achieves about 88.8% accuracy in the training data and about 88.9% in the test, well outperforming our baseline model. The L2 regularized model performs on par with our baseline, achieving a training accuracy of around 69.2% and a test accuracy of 66.9%.kNNWe next decided to try a different classification approach, specifically, the k-nearest neighbors model.# make kNN preds binarydef parsenKKRes(predictions):    for i in range(len(predictions)):        if predictions[i] &amp;lt; 0.5:            predictions[i] = 0        else:            predictions[i] = 1    return predictions# make regressorks = range(1, 100) # Grid of k'sscores_train = [] # R2 scoresscores_test = [] # R2 scoresacc_train = []acc_test = []for k in ks:    knnreg = KNeighborsRegressor(n_neighbors=k) # Create KNN model    knnreg.fit(x_train, y_train) # Fit the model to training data    scores_train.append(knnreg.score(x_train, y_train))  # Calculate R^2 score    scores_test.append(knnreg.score(x_test, y_test)) # Calculate R^2 score    predicted_train = knnreg.predict(x_train)    predicted_test = knnreg.predict(x_test)    acc_train.append(accuracy_score(y_train, parsenKKRes(predicted_train)))    acc_test.append(accuracy_score(y_test, parsenKKRes(predicted_test)))# Plotfig, ax = plt.subplots(1,2, figsize=(20,6))ax[0].plot(ks, scores_train,'o-')ax[0].set_xlabel(r'$k$')ax[0].set_ylabel(r'$R^{2}$')ax[0].set_title(r'Train $R^{2}$')ax[1].plot(ks, scores_test,'o-')ax[1].set_xlabel(r'$k$')ax[1].set_ylabel(r'$R^{2}$')ax[1].set_title(r'Test $R^{2}$')plt.show()# determine which k index has best test accuracyk_index = np.argmax(acc_test)print(&quot;[kNN] Classification accuracy for training set: &quot;, acc_train[k_index])print(&quot;[kNN] Classification accuracy for test set: &quot;, acc_test[k_index])[kNN] Classification accuracy for training set:  0.6314229249011858[kNN] Classification accuracy for test set:  0.6590909090909091Our kNN regressor performs at the same level as our baseline logistic classifier. The test set is at a 65.9% accuracy while the training is at 63.1%.LDA and QDAWe now consider discriminant analysis, which provides an alternative approach to classification.We will try both LDA and QDA and compare them.# LDAlda = LinearDiscriminantAnalysis()model_lda = lda.fit(x_train, y_train)acc_lda = model_lda.score(x_train, y_train)acc_lda_test = model_lda.score(x_test, y_test)# print accuracy scoresprint(&quot;[LDA] Classification accuracy for train set :&quot;,acc_lda)print(&quot;[LDA] Classification accuracy for test set :&quot;,acc_lda_test)# QDAqda = QuadraticDiscriminantAnalysis()model_qda = qda.fit(x_train, y_train)acc_qda = model_qda.score(x_train, y_train)acc_qda_test = model_qda.score(x_test, y_test)print(&quot;[QDA] Classification accuracy for train set:&quot;,acc_qda)print(&quot;[QDA] Classification accuracy for test set:&quot;,acc_qda_test)[LDA] Classification accuracy for train set : 0.8809288537549407[LDA] Classification accuracy for test set : 0.8843873517786561[QDA] Classification accuracy for train set: 0.8656126482213439[QDA] Classification accuracy for test set: 0.866600790513834LDA performs better than QDA, and both perform above baseline. LDA achieves an accuracy of about 88.1% in the training and 88.4% in the testing data, while QDA ahieves an accuracy of about 86.6% in the training and 86.7% in the testing data.Decision TreesThe next type of decision model we are interested in is the decision tree.We will first create a simple tree.# classify by depthdef treeClassifierByDepth(depth, x_train, y_train, cvt = 5):    model = DecisionTreeClassifier(max_depth=depth).fit(x_train, y_train)    return cross_val_score(model, x_train, y_train, cv = cvt)# 5-fold CVmeans = []lower = []upper = []sds = []trains = []for i in range(1, 20):    # fit model    tc = treeClassifierByDepth(i, x_train, y_train)    # calc mean and sd    cur_mean = np.mean(tc)    cur_sd = np.std(tc)    train_val = DecisionTreeClassifier(max_depth=i).fit(x_train, y_train).score(x_train,y_train)    # add to lists    trains.append(train_val)    means.append(cur_mean)    lower.append(cur_mean - 2*cur_sd)    upper.append(cur_mean + 2*cur_sd)    plt.plot(range(1,20),means)plt.fill_between(range(1,20), lower, upper, alpha = 0.3, label = &quot;Mean CV score (+/- 2SD)&quot;)plt.plot(range(1,20), trains, label=&quot;Train&quot;)plt.title(&quot;Spotify Playlist Decision Tree Model Estimated Performance&quot;)plt.xlabel(&quot;Maximum Depth&quot;)plt.ylabel(&quot;Score&quot;)plt.legend()plt.show()# cross validation performancetrain_score = means[5]print(&quot;[Decision Tree Classifier] Mean classification accuracy training set: &quot;,train_score)print(&quot;Mean +/- 2 SD: (&quot;, lower[4],&quot;,&quot;,upper[4],&quot;)&quot;)[Decision Tree Classifier] Mean classification accuracy training set:  0.8796923499519297Mean +/- 2 SD: ( 0.8649746226416641 , 0.8909557288057866 )# test set performancemodel_dec_tree = DecisionTreeClassifier(max_depth=6).fit(x_train, y_train)test_score = model_dec_tree.score(x_test, y_test)print(&quot;[Decision Tree Classifier] Mean classification accuracy test set: &quot;, test_score)[Decision Tree Classifier] Mean classification accuracy test set:  0.8903162055335968We achieve the best cross-validation score at a tree depth of 6, with an accuracy of 88.0%. Additionally, we observe a relatively narrow spread in estimated performances, as there is a roughly 2% difference between +/- two standard deviations. We see that this model also performs quite well in the test set, with an accuracy score of 88.7%, proving superior to all the other models we have tried so far.BaggingNow, we will consider ensemble methods that improve upon our simple decision tree.The first one we try is bagging: we create 45 bootstrapped datasets, fitting a decision tree to each of them and saving their predictions:# bootstrapbagging_train_arr = []bagging_test_arr = []estimators = []tree_res = []tree = DecisionTreeClassifier(max_depth=new_depth)# classify train and test with bootstrap modelsfor i in range(num_trees):    boot_x, boot_y = resample(x_train, y_train)    fit_tree = tree.fit(boot_x, boot_y)    estimators.append(fit_tree)    bagging_train_arr.append(tree.predict(x_train))    bagging_test_arr.append(tree.predict(x_test))Construct dataframes with all the bootstrapped data:# trainbagging_train = pd.DataFrame()for i in range(len(bagging_train_arr)):    col_name = &quot;Bootstrap Model &quot; + str(i + 1)    bagging_train[col_name] = bagging_train_arr[i]# testbagging_test = pd.DataFrame()for i in range(len(bagging_test_arr)):    col_name = &quot;Bootstrap Model &quot; + str(i + 1)    bagging_test[col_name] = bagging_test_arr[i]    # generate renaming row objrename = {}for i in range(0, 1104):    rename[i] = &quot;Training Row &quot; + str(i + 1)bagging_train.rename(rename, inplace=True)bagging_test.rename(rename,  inplace=True)Combine predictions from all the bootstraps and assess how the model performs:# combining all data points from the data to determine accuracyy_preds_train = []y_preds_test = []for row in bagging_train.iterrows():    if np.mean(row[1]) &amp;gt; 0.5:        y_preds_train.append(1)    else:        y_preds_train.append(0)for row in bagging_test.iterrows():    if np.mean(row[1]) &amp;gt; 0.5:        y_preds_test.append(1)    else:        y_preds_test.append(0)        def compare_acc(preds, actual):    count = 0    for i in range(len(preds)):        if preds[i] == actual.item(i):            count += 1    return(count/len(preds))bagging_train_score = compare_acc(y_preds_train,y_train)bagging_test_score = compare_acc(y_preds_test,y_test)print(&quot;[Bagging] Classification accuracy for train set: &quot;, bagging_train_score)print(&quot;[Bagging] Classification accuracy for test set: &quot;, bagging_test_score)[Bagging] Classification accuracy for train set:  0.9370059288537549[Bagging] Classification accuracy for test set:  0.9150197628458498The model clearly performed better after using bootstrapped data to fit it. It has increased from 88% on the training data to 94.0%, and from 88.1% on the test data to 90.4%. This makes bagging the most accurate model we have tried so far.Random ForestOur next ensemble method is random forest, which randomly subsets predictors upon which to generate decision trees.# config parametersnum_trees = 45new_depth = 6# model random forestmodel_rf = RandomForestClassifier(n_estimators=num_trees, max_depth=new_depth)# fit model on X_train datamodel_rf.fit(x_train, y_train)# predict using modely_pred_train_rf = model_rf.predict(x_train)y_pred_test_rf = model_rf.predict(x_test)# accuracy from train and testtrain_score_rf = accuracy_score(y_train, y_pred_train_rf)test_score_rf = accuracy_score(y_test, y_pred_test_rf)# print accuracy scoresprint(&quot;[Random Forest] Classification accuracy for train set: &quot;, train_score_rf)print(&quot;[Random Forest] Classification accuracy for test set:&quot;, test_score_rf)[Random Forest] Classification accuracy for train set:  0.9300889328063241[Random Forest] Classification accuracy for test set: 0.9229249011857708A random forest, at the same depth as the decision tree (namely a depth of 6) performs even better. The test data reaches an accuracy of about 92.6% in the training at 91.5% in the test.BoostingFinally, we will consider boosting, an iterative approach that might eliminate some more of the error in our trees.# define classifier functiondef boostingClassifier(x_train, y_train, depth):    # AdaBoostClassifier    abc = AdaBoostClassifier(DecisionTreeClassifier(max_depth=depth),                         n_estimators=800, learning_rate = 0.05)    abc.fit(x_train, y_train)    # staged_score train to plot    abc_predicts_train = list(abc.staged_score(x_train,y_train))    plt.plot(abc_predicts_train, label = &quot;train&quot;);    # staged_score test to plot    abc_predicts_test = list(abc.staged_score(x_test,y_test))    plt.plot(abc_predicts_test, label = &quot;test&quot;);    plt.legend()    plt.title(&quot;AdaBoost Classifier Accuracy, n = &quot;+str(depth))    plt.xlabel(&quot;Iterations&quot;)    plt.show()        return(&quot;Maximum test accuracy for depth of &quot;+str(depth)+&quot; is &quot;+str(max(abc_predicts_test))+&quot; at &quot;+str(abc_predicts_test.index(max(abc_predicts_test)))+&quot; iterations&quot;)for i in range(1,5):    print(boostingClassifier(x_train, y_train, i))Maximum test accuracy for depth of 1 is 0.9150197628458498 at 773 iterationsMaximum test accuracy for depth of 2 is 0.9298418972332015 at 751 iterationsMaximum test accuracy for depth of 3 is 0.9268774703557312 at 500 iterationsMaximum test accuracy for depth of 4 is 0.9219367588932806 at 530 iterationsWe see based upon an AdaBoostClassifier the maximum test accuracy of 93.0% is attained at a depth of 2. This is attained after 751 iterations. The AdaBoostClassifier is our most accurate model so far.Neural NetworksFinally, we created an artificial neural network to classify our playlist songs.# check input and output dimensionsinput_dim_2 = x_train.shape[1]output_dim_2 = 1print(input_dim_2,output_dim_2)14 1# create sequential multi-layer perceptronmodel2 = Sequential() # initial layermodel2.add(Dense(10, input_dim=input_dim_2,                  activation='relu')) # second layermodel2.add(Dense(10, input_dim=input_dim_2,                  activation='relu'))# third layermodel2.add(Dense(10, input_dim=input_dim_2,                  activation='relu'))# output layermodel2.add(Dense(1, activation='sigmoid'))# compile the modelmodel2.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])model2.summary()# fit the modelmodel2_history = model2.fit(    x_train, y_train,    epochs=50, validation_split = 0.5, batch_size = 128, verbose=False)# model lossprint(&quot;[Neural Net - Model 1] Loss: &quot;, model2_history.history['loss'][-1])print(&quot;[Neural Net - Model 1] Val Loss: &quot;, model2_history.history['val_loss'][-1])print(&quot;[Neural Net - Model 1] Test Loss: &quot;, model2.evaluate(x_test, y_test, verbose=False))print(&quot;[Neural Net - Model 1] Accuracy: &quot;, model2_history.history['acc'][-1])print(&quot;[Neural Net - Model 1] Val Accuracy: &quot;, model2_history.history['val_acc'][-1])[Neural Net - Model 1] Loss:  7.79790558079957[Neural Net - Model 1] Val Loss:  8.034205742033103[Neural Net - Model 1] Test Loss:  [7.719139232937055, 0.5158102769154334][Neural Net - Model 1] Accuracy:  0.5108695654529828[Neural Net - Model 1] Val Accuracy:  0.49604743024106085Our initial accuracy isn’t great. We achieve an accuracy of 48.9% in the training and 50.4% in the validation, and an accuracy of 48.4% in the test. Let’s see if we can improve our network to fit the data better.# create sequential multi-layer perceptronmodel3 = Sequential() # Hidden layersfor i in range(40):    model3.add(Dense(10, input_dim=input_dim_2,         activation='relu')) # output layermodel3.add(Dense(output_dim_2, activation='sigmoid'))# compile the modelmodel3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])model3.summary()# fit the modelmodel3_history = model3.fit(    x_train, y_train,    epochs=300, validation_split = 0.1, batch_size = 128, verbose=False)# model lossprint(&quot;[Neural Net - Model 2] Loss: &quot;, model3_history.history['loss'][-1])print(&quot;[Neural Net - Model 2] Val Loss: &quot;, model3_history.history['val_loss'][-1])print(&quot;[Neural Net - Model 2] Test Loss: &quot;, model3.evaluate(x_test, y_test, verbose=False))print(&quot;[Neural Net - Model 2] Accuracy: &quot;, model3_history.history['acc'][-1])print(&quot;[Neural Net - Model 2] Val Accuracy: &quot;, model3_history.history['val_acc'][-1])[Neural Net - Model 2] Loss:  0.6267417644590524[Neural Net - Model 2] Val Loss:  0.6291195959220698[Neural Net - Model 2] Test Loss:  [0.6115785545040026, 0.6432806319398843][Neural Net - Model 2] Accuracy:  0.625857809154077[Neural Net - Model 2] Val Accuracy:  0.6197530875971288Even after changing hyperparameters, our neural network does not perform very well. Using 40 layers and 300 epochs, the accuracy in the training data is still 62.8% while the accuracy in the test is 65.2%. This is baffling, because we expected our neural network to perform very well. Perhaps this mediocre perforance is due to limitations of our data set (only 14 features and &amp;lt;5000 songs), or of the specific methods we used.Model SelectionBased upon the presented analysis, we conclude that our boosted decision tree classifier, at a depth of 2 with 751 iterations, is the best model. It achieves the highest accuracy in the test set, of 93.0%.",
    "url": "http://localhost:4000/final_notebook/models.html",
    "relUrl": "/final_notebook/models.html"
  }
}

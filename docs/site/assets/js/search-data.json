{
  "0": {
    "id": "0",
    "title": "Background",
    "content": "Background  Project Question  Project Motivation Approach Literature Review How do President Trump’s tweets impact the volatility of the S&P 500? How has that impact changed over time, if at all, since he began his term as U.S. President? In September 2019, J.P. Morgan launched their ‘Volfefe’ index meant to track U.S. treasury bond volatility based on market sentiment changes due to unpredictable tweets from President Trump. Using a portion of Trump’s tweets since 2017, JPM developed a model that predicted bond market volatility surprisingly accurately. In our project, we examine VIX, the first benchmark index to measure expectations of future market volatility. VIX prices are based on S&P 500 options, and rapid and/or significant price changes usually reflect sudden changes in market sentiment. For example, if VIX jumps 10% in a given day, it suggests that investors are generally more unsure about the future of the U.S. economy, resulting in greater volatility in the overall market. We had three goals with this project: create a well-performing model that can predict changes in VIX based on Trump’s tweets use that model to generate positive returns in the coming future produce a clean interface using jekyll to display our project analysis and results so that others can replicate our method and hopefully improve it! Approach UPDATE We first downloaded Trump’s twitter database and cleaned the data to focus on the tweet content. Then, we analyzed it using textblob to generate a sentiment score for each tweet. Next, we went to HBS and manually pulled minute-by-minute VIX data (a cumbersome process, but one we believe was worth the effort!). Then, we consolidated this data into a single dataframe. We then randomly split this data into a training and test set with the outcome variable being the change in VIX pricing and the core explanatory variable being the sentiment score for a given tweet, along with several other predictors constructed based on the twitter and VIX data. Predicting the absolute VIX price yielded irrelevant results (nonsurprisingly), but predicting the change in VIX price produced some interesting results. Next, we built and fit a variety of classifiers with differing VIX price change time intervals. Models we built include: Baseline Model Logistic Classifier For each model, we evaluated its accuracy on both our training and and test set. Based on accuracy scores, we determined the classifier with the highest performance on the test set. Finally, we ran our best-performing model on a fresh set of songs and asked Grace if she liked her new playlist. Literature Review Using Twitter to Predict the Stock Market Attemt to extract mood levels from Social Media applications in order to predict stock returns Uses roughly 100 million tweets that were published in Germany between January, 2011 and November, 2013 Find that it is necessary to take into account the spread of mood states among Internet users Portfolio increases by up to 36 % within a sixmonth period after the consideration of transaction costs Data Mining Twitter To Predict Stock Market Movements Sentiment analysis of Twitter data from July through December, 2013 Attempt to find correlation between users’ sentiments and NASDAQ closing price and trading volume Find that “Happy” and “sad” sentiment variables’ lags are strongly correlated with closing price and “excited” and “calm” lags are strongly correlated with trading volume Incorporates interesting word weighting to classify tweets into emotional groupings A topic-based sentiment analysis model to predict stock market price movement using Weibo mood Demonstrate that the emotions automatically extracted from the large scale Weibo posts represent the real public opinions about some special topics of the stock market in China. Nonlinear autoregressive model, using neural network with exogenous sentiment inputs is proposed to predict the stock price movement. “Given the performance, if more related topics for a stock are found out, the accuracy could be higher considering more completed topic-based sentiment inputs.” Recognize that the time lag on sentiment makes it difficult to fully predict market changes Can Twitter Help Predict Firm-Level Earnings and Stock Returns? Test whether opinions of individuals tweeted just prior to a firm’s earnings announcement predict its earnings and announcement returns Find that the aggregate opinion from individual tweets successfully predicts a firm’s forthcoming quarterly earnings and announcement returns",
    "url": "http://localhost:4000/background.html",
    "relUrl": "/background.html"
  },
  "1": {
    "id": "1",
    "title": "Conclusions",
    "content": "Conclusions TABLE OF CONTENTS Analysis of Best Model Limitations and Future Work Final Thoughts Analysis of Best Model Our best model is the boosted decision tree classifier with a depth of 10 and 5 iterations, which performs with an accuracy of 66.8% in the test set. The following table summarizes the top 10 features in our best model: feature	        importance minutes_y	        0.149895 30_day_perc	      0.126930 14_day_perc	      0.097110 Last Price	      0.094082 24_hr_perc	      0.087268 7_day_perc	      0.071768 tweet_len	        0.071687 sent_score	      0.059620 month	            0.057562 hour	            0.04439 These feature importances on our best model (ADA Boosting with time interval 60 minutes) demonstrates counterintuitively that the most important feature to predict the movement of VIX data post a trump tweet is what time he tweeted. The important thing to note is that VIX trades global hours, so the trading activity begins 3:15am and ends 4:15pm (with a 15 minute break between 915am and 930am). This suggests two possible factors for the importance of the time Trump tweets. Firstly, if the time Trump tweets is non-US market hour times, then the immediate impact (in the 60 minute time interval) is less profound because while there will be some traders looking to trade based on his tweets even outside US Market Hours, it is likely that most of the traders that closely trade on Trump’s actions are based in US and that most of them are trading during Market hours (the argument is not that there are not traders in China/other places trading off Trump’s tweets/actions, but that the proportion of them is much higher in US which ultimately is reflected more in the VIX). Secondly, on similar lines, this also would be further demonstrated if we had access to Trading Volume minute by minute. Some hours are more likely to have higher trading volume than other times, so the opportunity for a Trump tweet to cause significant impact on Volatility would also be determined by the size of the trading volume at that given time. However, this was incredibly challenging to collect because VIX does not have trading volume data itself, but rather we would have had to collect minute by minute volumes from the put and call option derivates on the S&P, which was not compiling cleanly with the VIX minute by minute data that we had collected from the Bloomberg Terminals at HBS Baker Library. Also, note that the way our minutes variable is constructed technically precludes the need for an hours variable, something that we could have changed in order to make our model ‘cleaner.’ Additionally, the next three predictors all have to do with the momentum of the VIX prices, rather than the tweets themselves. This suggests two potential things. Firstly, VIX data is influenced by combination of things and even if we look so closely just sixty minutes after the tweet, there are still market forces that are a continuing trend that then combine with the Trump tweet effect to influence prices on VIX. Tweet Length is incredibly interesting, as it probably indicates that a longer tweet likely (though not always) suggests that this tweet is more substantive with bigger implications thus causing further movement on the trading activity. We then continue with further VIX data points with momentum related predictors and time (now, in the form of what month it was tweeted). Although not as much as would have thought initially, but sentiment score has some quantifiable significance to the VIX Data. We predict that the reason for this is because his positive tweets are not necessarily market reassuring to lead to VIX data decreasing nor are his negative tweets necessarily acted upon by the market even if they have content that is related to the market, as he has in the past often threatened no deal which would be classified as a negative sentiment, but the market may ignore this and other factors impacting the VIX at that time, making sentimental scores not as powerful as initially predicted. Moreover, in hindsight, we think a way to control for this is to create interaction terms with keywords such as ‘China’ and ‘Deal’. On the topic of keywords, we were surprised for a second that any of our keyword related predictors were not present in our top features. We realized that the keywords were separate one-hot-encoded predictors such that any keyword itself did not have enough data-points to consistently predict the VIX Data, but this in hindsight would have been really helpful to create multiple interaction terms among several different keywords and all the keywords to observe if they are then found more appealing by the model. Limitations and Future Work Data Size We generated a dataset by consolidating and cleaning the Trump’s tweets since he became the presumptive GOP candidate in 2016. It would be interesting to expand our data to before he was President/in the running and see how the effects of his Tweets changed if all. We could also rn variations of our analysis using keyword-based subset of the Twitter data to see if there is interesting sentiment analysis in these specific Tweets Trumps posts. Improve Neural Network Ideally, we would further optimize the Neural Network model as its accuracy is currently behind ADA Boosting by about 6-7% points. We tried several different layers, regularization techniques, optimizers, batch-sizes, etc. Unfortunately, we kept leveling around the 60-62% range in our accuracy. We also adapted the data we were training the model for, as the target outcome no longer had three categories, but instead only two (just positive and negative as our threshold previously was already very low at 0.01), as our binary_cross_entropy was working better than the respective loss function for the multi-classification model (in the sense that latter was outputting ridiculously low accuracy scores). International Markets We would also have benefitted from incorporating price and trading data from other markets—most notably, the Chinese stock market. Trump’s tweets, particularly tweets related to the trade war with China, are likely followed closely by traders there and would have a market-moving effect. Further, adding Chinese trading hours to our dataset would have allowed us to broaden the range of tweets that could be incorporated into our predictive model. It is also possible that the impact of trade-related tweets would be easier to isolate with data on the Chinese market, because the subset of Trump tweets that plausibly move stock prices in China would be narrower than those that have an impact on American shares. This reduces the possibility that our China trade tweet signals would be confounded by tweets containing those signals that are not related to the Chinese trade issue. Also, the actual market movements would be sharper, insofar as evidence suggests that the current trade dispute has a more negative impact on Chinese equities than it does on American equities. Sentiment Analysis Improvement A review of the relevant literature highlighted the importance of clarifying and classifying sentiment more sepcifically. We could improve this model by loading specific emotion datasets to improve our sentiment analysis and take scoring beyond just positive and negative via textblob’s fundamental functionality. Utilizing these libraries could help make our sentiment analysis more robust. This would be particularly important given sentiment score was not the most important predictor in our model. Regression Functional Form As briefly mentioned above, we could improve the interpretation and usefulness of the model by including interaction terms, particularly incorporating some of the indicator variables. Additionally, we could try to include quadratic terms to see how that changes the effects of certain variables in our analysis. Final Thoughts Overall, we were proud that we were able to improve our model such that test accuracy was not only above 50%, but also that it reached nearly 70%. Additionally, the fact that our model successfully reached this accuracy level for our 60 min interval offers a great trading opportunity based on the VIX index. Based on the above comments, though, we do believe more tweaking should be done in order to improve the model and make it more robust if it were to be using in an actual trading context.",
    "url": "http://localhost:4000/conclusions.html",
    "relUrl": "/conclusions.html"
  },
  "2": {
    "id": "2",
    "title": "Data Exploration",
    "content": "Data Exploration TABLE OF CONTENTS Data Collection and Cleaning Twitter VIX Consolidated Data Data Description Exploratory Data Analysis Data Collection and Cleaning Twitter We pulled all of Trump’s tweets in the last few years from his Twitter archive. Due to the naature of the database, we spent much time cleaning this data for our purposes. We took the following steps to clean the data: Importing raw data from the archive Removing unnecessary columns Adjusting GMT to Eastern Time + accounting for daylight savings Manually fixing errors in cells where the delimiting was incorrectly done in the database output and manually re-inserting the delimiting character First, we removed all Twitter data preceding June 1, 2016. Trump became the presumptive nominee of the Republican Party that summer, and we figured that this represented the beginning of Trump tweets’ meaningful market relevance. There remained numerous instances of tweet data from the database lumping together multiple tweets in a single entry. We noticed that when this occurred, the components of these tweets were delimited by the character “{“. We used Excel’s “find” function to filter for all such cases. We then transferred all these “mega-cells” into a separate Excel sheet, which automatically separated the subcomponents of the tweet data: each row represented the data, including the actual tweet string, for a given tweet. However, the various classes of data (e.g., text, id_str, date, etc.) were lumped together in one column. These were split using the aforementioned delimiter. We appended these cleaned data into the original Excel sheet, and then sorted the entire sheet by date to complete the process. Additionally, tweets that were subject to the above issue—and many others, in general—also misclassified retweets as Trump’s original tweets. Retweets started with ‘@[handle] : ‘ so I wrote a function to filter through the data for entries starting with ‘@‘ that also included a ‘:’. Not all such instances were retweets—some were just tweets by Trump—and so this process had a manual component too. We utilized ntlk’s textblob function in order to analyze the sentiment of tweets in our data set. For each tweet, this function created a polarity score (the more positive a tweet is, the closer the score is to 1; the more negative, the closer it is to -1). The function also returns a subjectivity score. Lower subjectivity score means that the tweet’s polarity score more objectively represents its sentiment. VIX VIX is the first benchmark index to measure expectations of future market volatility (based on S&P 500 options). Since Milestone 2, we have procured minute-by-minute VIX data from 12/2015 - 11/11/2019. This data includes only VIX pricing on trading days throughout the past few years (excludes holidays, weekends, etc.). We manually pulled the data from a Bloomberg Terminal at HBS Baker Library. Given the size of the dataset and the Terminal download limits, we manually copied and pasted the VIX data directly into a csv file. Note that all other sources of VIX data are at best day-by-day and typically cost a nontrivial amount. VIX is managed by CBOE (Chicago Board Options Exchange). The global trading hours for VIX can be found here. Trading hours range from 3:15 am EST until 4:15 pm EST. There is a break bewteen 9:15 and 9:30 am, but this is addressed in how we consolidate our data. Consolidated Data In order to consolidate the data, we merge our Twitter and VIX dataframes on date/time to ensure that we are only looking at tweets that are posted during VIX trading hours. That way, we do not have to worry about tweets occurring outside these hours. Also, when we look at the change in VIX price, we are only looking at changes during trading hours, so examining only tweets that are posted during trading hours allows us to perform this analysis soundly. from textblob import TextBlob tweets = twitter_archive_df['text'] tweets_list = [tweet for tweet in tweets] big_tweet_string = ' '.join(tweets_list) tokens = word_tokenize(big_tweet_string) words = [word.lower() for word in tokens if word.isalpha()] stop_words = set(stopwords.words('english')) words = [word for word in words if not word in stop_words] scores = [] subjectivity = [] for tweet in tweets_list: blob_tweet = TextBlob(tweet) sentiment = blob_tweet.sentiment score = sentiment[0] subject = sentiment[1] scores.append(float(score)) subjectivity.append(float(subject)) twitter_archive_df['sent_score'] = scores twitter_archive_df['subjectivity'] = subjectivity # twitter_archive_df.head() Data Description Our data includes the following features: price_delta, price_delta_5, … , price_delta_60: These variables assisted us in the model-building process, by contributing to our understanding of how VIX digested the information contained within a Trump tweet. price_delta was the next-minute change in the VIX, price_delta_5 was the change 5 minutes following the tweet, and so on. Naturally, these cannot be predictors in our model. However, by including these as features in the random forest, and then assessing their importance, we were able to determine what portion of VIX movement, say, 15 minutes following a Trump tweet could be explained by the index’s movement in the first 10 minuets. We ultimately determined that the market absorbs the impact of the tweet within the first 5-10 minutes of the tweet’s posting. That is to say, price_delta_5 was a hugely important predictor of price_delta_10. This suggested to us that our predictive efforts should seek to predict the VIX’s change after >10 minutes have passed, so as to allow the impact of the tweet to register. (But not too much longer, for then confounding factors may enter the model.) Last Price: This variable represents the last recorded price in the VIX index at the time of the tweet. is_retweet: This variable describes whether a given tweet is a retweet or not. We re-coded the boolean from the database as a binary value, with is_retweet = 1 representing a tweet that Trump retweeted, while zero denotes a Trump tweet. This variable is important because we might expect Trump’s own tweets to have a greater market impact than his retweets. 24_hr_perc, 7_day_perc, 14_day_perc, 30_day_perc: These variables describe the change in the value of VIX over the preceding 24hrs, 7 days, 14 days and 30 days. The intuition behind including this is that the direction and strength of volatility’s momentum could well be tied to the market impact of a tweet. If, for instance, volatility has dropped because the market has become somewhat complacent about trade-related risks (i.e., the percentage change in VIX is negative), then a Trump tweet voicing dissatisfaction about the progress of a trade deal would cause a much larger spike in volatility than if markets had already “priced in” the possibility of antagonistic chatter from Trump. minutes: This variable is the minute in the day that the tweet was made. There is research that shows the majority of trading is done near the market’s opening and close. From this follows that we may expect tweet’s made around this time to have greater predictive impact than those made during the market’s slower hours. hour_0 to hour_23: These are dummy variables for the hour in the day that the tweet was made. The intuition for this is similar to that for ‘minutes’; perhaps the former is too granular, in which case the hour dummies might have more meaningful predictivity. month_1 to month_12: These are dummy variables for the month in which the tweet was posted, with month_1 representing January. Perhaps some months are more susceptible to market volatility, perhaps due to increased trading volumes or perhaps because those months are heavy with corporate earnings. That would potentially magnify the impact of tweets made in those months. year: This is the year in which the tweet was posted (after escalation of trade war, after inaugration). This would be a proxy for tweets made, say, before and after the escalation of trade war, or before and after the inaugration. We would expect that the impact of Trump tweets would vary depending on the circumstances of the particular year. keywords (contained in all_keywords): These are all words that could be associated with the US-China trade war. By creating a column for each keyword, we turn each one into a predictor, allowing us to assess the impact on the VIX index of the presence of that particular keyword. Each row in a given column notes the number of times that particular keyword is used in that tweet. This has been coded to be case-insensitive–important given Trump’s occasionally unconventional capitalization pracatices. The usage of one of these trade-related keywords could make a Trump tweet particularly meaningful to the market. keyword_mentions: This is the total number of non-unique keyword mentions in a given tweet. That is to say, if ‘trade’ is mentioned 2 time, ‘China’ 3 times and ‘deficit’ 1 time, then keyword_mentions=6. We think it is plausible that increased used of keywords would be associated with a tweet that contains more substance on a potentially market-moving matter. handle_mentions: This is the number of other Twitter users mentioned in the tweet, as measured by the number of ‘@’ mentions in the tweet string. A potential issue with this would be extraneous uses of the character ‘@’ or the mentioning of email addresses. We can assume that neither problem would occur much in the dataset. Tweets that mention multiple other users are perhaps less likely dealing with macroeconomic issues–maybe they’re related to personal interactions–so this predictor might be negatively related to market impact. exclamations: This variable measures the number of exclamation marks used in each tweet string. This could have predictive impact on the VIX because Trump’s use of emphasis on a tweet regarding, say, how China is “ripping off” America, might indicate to the market a greater leaning toward anti-China trade policies. links: This is the number of links in a Trump tweet, as measured by occurrences of the ‘http’ string. This may not have the most market importance, but it’s possible that the presence of links suggests that Trump is making some kind of comment with reference a particular news story or article, as opposed to making a pronouncement on a general issue of policy–implying reduced market impact. tweet_len: This is the length of a tweet. The logic underpinning the inclusion of this predictor is that longer tweets are likelier to be more substantive, and more substantive tweets are likelier to have an impact on the market. Exploratory Data Analysis We did an initial analysis of some keywords, as shown in our notebook. The list is ['trade', 'Trade', 'deal', 'products', 'manufacturing', 'China', 'Xi', 'Xi Jinping', 'CCP', 'Communist Party', 'Beijing']. Approximately 1/10 of Trump’s tweets that we cleaned (of ~14,000 total) contain some combination of these keywords. As we see above, the majority of the ‘keywords’ (based on our initial keyword list) appearing in our data are trade, Trade, deal, China, and Xi (products and manufacturing not far behind). CCP, Communist Party, and Beijing do not show up that frequently. In the future, we will probably employ functionality to search for the most frequently appearing useful words (excludes articles, punctuation, etc.). The histogram is reproduced below: The goal of looking at this list of keywords is to hone in on the content of Trump's tweets, specifically words and tweets in which we utilizes a negative tone toward China, and use tweet characteristics related to these keywords as relevant predictors in our model (such as how many of these keywords appear in a given tweet). Now, we look at the distribution of sentiment and subjectivity scores for our Twitter data. We see that most of the tweets, as classified by textblob, are mainly neutral and leaning positive if anything. Then, in terms of subjectivity, we see that the distribution is almost normal with a great chunk of tweets being objectively classified based on sentiment score. Below, we will create a sentiment score variable that is weighted by (1-subjectivity) to capture 'objective' sentiment score. Next, we examine the VIX data. Below are summary statistics for the data we were able to pull and consolidate from Bloomberg. Last Price	price_delta	price_delta_5	24_hr_perc	7_day_perc	14_day_perc	30_day_perc	52_week_high	52_week_perc	vix_delta_sign	year	month	hour	minutes count	722237.000000	722237.000000	722237.000000	721833.000000	719409.000000	716581.000000	710294.000000	676514.000000	676514.000000	722236.000000	722237.000000	722237.000000	722237.000000	722237.00000 mean	14.738460	-0.000022	-0.000087	0.001939	0.010396	0.016207	0.022988	38.205824	0.399478	0.000267	2017.472323	6.566354	9.413017	594.44071 std	4.103039	0.062353	0.125227	0.067065	0.163789	0.218262	0.275639	11.517582	0.103463	0.355108	1.113672	3.335537	3.796773	226.93971 min	8.910000	-12.690000	-12.730000	-0.430000	-0.560000	-0.610000	-0.610000	17.280000	0.210000	-1.000000	2015.000000	1.000000	3.000000	195.00000 25%	11.910000	-0.010000	-0.030000	-0.030000	-0.080000	-0.100000	-0.130000	26.720000	0.330000	0.000000	2017.000000	4.000000	6.000000	395.00000 50%	13.680000	0.000000	0.000000	0.000000	-0.010000	-0.020000	-0.020000	36.200000	0.400000	0.000000	2017.000000	7.000000	10.000000	610.00000 75%	16.580000	0.010000	0.030000	0.030000	0.070000	0.090000	0.110000	50.300000	0.450000	0.000000	2018.000000	9.000000	13.000000	791.00000 max	50.200000	8.570000	8.520000	1.490000	2.570000	3.320000	3.920000	53.290000	1.000000	1.000000	2019.000000	12.000000	16.000000	974.00000 Just based on the above, we can see a pretty large range in the values of the VIX, suggesting that the pricing jumps around a lot. At the same time, though, we see that on a minute by minute basis, the price change is very small. We keep this in mind when developing our models because even though are outcome is price delta, we experiment with different time intervals over which that delta is calculated to see which model will be most useful. Additionally, in our EDA, we looked at a snippet of the VIX pricing data. Our thought process was before diving into any analysis, we should first determine when the VIX rose or fell significantly and see if those changes appear to be related to any significant news events around those times. Our initial graph of the VIX data looks at EOD VIX pricing over a set period of time (December 1, 2015 - December 1, 2016). (Our analysis will hone in on minute by minute, but for us, it is important to be aware of the major surges). This graph is not reproduced below because of its size, but it can be seen in the notebook (title: “VIX Pricing over time”). The following were the main surges during this time period: 12/11/15 1/19/16 2/11/16 6/14/16 6/27/16 9/12/16 Two weeks before 2016 Presidential elections and a week afterwards For each date, we cross referenced it with the Financial Times Archives for the few days before and a few days after. Note: VIX is incredibly volatile, where for most metrics/indices a 3-4% change on any given day is regarded as a big move, such changes are the norm for VIX. Thus explaining any major surge due to one event remains challenging, yet for a high level understanding, we believe it to be necessary. Oil Prices Reaching Seven Year Lows Wall St. makes worst start to year, global bearishness, oil resumes slide, Fed might raise interest rates again Very mixed news, articles suggest renewed China risks due to China’s unruly peer-to-peer lending — 21 people arrested involved in “a complete Ponzi scheme” — ballooned in size last year as credit-starved private companies paid swingeing interest rates to secure loans Weak jobs data, major uncertainty regarding Fed hiking interest rates Post-Brexit jitters Oil price hikes, Merkel loses major state election in response to her open-door refugee policy Three major moves: first, as polling data shows narrowing of Clinton’s lead, the VIX continuously climbs, one of its more significant spikes being the day Fed reopened the investigation into her emails, secondly, one of VIX’s more significant downturns (though, still mild relative to the climbs of the previous weeks) was when the investigation was officially closed, and thirdly when Trump won, the VIX surged upwards, only to calm the day after when trading resumed, which was the exact trend following the Brexit vote. Note that the moves above are not directly related to Trump's tweets, but they did give us a sense for how striking news could affect VIX pricing on a minute by minute basis.",
    "url": "http://localhost:4000/final_notebook/data.html",
    "relUrl": "/final_notebook/data.html"
  },
  "3": {
    "id": "3",
    "title": "Home",
    "content": "Trump Tweet VIX (Group 3) TABLE OF CONTENTS Overview Website Navigation About Us Overview For our final project, we wanted to build a model that uses President Trump’s tweets to predict changes in the VIX Index. Theoretically, a model performing accurately 51 (or higher) percent of the time provides a way to intelligently bet on market volatility and generate a positive return on average. We utilized Trump’s Twitter archive here to build our database of tweets, and we pulled minute-by-minute VIX data from an HBS Baker Library Bloomberg Terminal. Our analysis begins in May 2016 when Trump emerged as a solid, leading candidate for the GOP in the 2016 Presidential election. insert info about models Source: CNN Website Navigation Our project background and question, as well as a brief literature review, can be found on the background page. Our data description and collection information, as well as our exploratory data analysis, can be found on the data exploration page. Our final models can be found on the models page. Our results, analysis, conclusions, and final discussion can be found on the conclusions page. About Us We are Gaurang Goel, Yashvardhan Bardoloi, and Adil Bhatia, Group 3 in CS109A, Fall 2019. Special thanks to Robbert Struyven and James Zeitler for their assistance.",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  },
  "4": {
    "id": "4",
    "title": "Models",
    "content": "Models TABLE OF CONTENTS Setup Baseline Model - Logistic L1 and L2 Regularization Random Forest Boosting Neural Networks Model Selection Setup Our goal for this project is to build a model that can accurately predict changes in VIX prices after Trump tweets. In the model, we use characteristics of the VIX data (like Last Price) and multiple characteristics of the Twitter data. Also, some predictors are common to both datasets, such as date/time predictors. Based on these predictors, we initially considered two types of models. First, we considered one that predicts absolute price changes in VIX pricing (continuous outcome). Second, we considered one that predicts the sign of VIX pricing changes (positive, negative, or no change - a categorical outcome). Intuitively, given what we know about the VIX and its pricing fluctuations, we thought it made more sense to focus on a model that predicts the change in pricing. Thus, we built a model that has a categorical output variable: -1 for negative price delta, 0 for no price delta, 1 for positive price delta. We also wanted to create interval versions of this model that looks at the VIX price change over 1 minute, 5, 10, 20 , 30, and 60 minutes. We wanted to consider all these options to determine the most accurate and most useful model. Realistically, if our model predicts well 10+ minutes after the tweet, it can offer a great chance to earn positive returns by trading on the VIX index. Another important consideration is the threshold at which we declare an outcome positive or negative. Given we are using minute-by-minute data, we decided upon a 0.001 threshold so that any change between -0.001 and 0.001 inclusive should be categorized as 0 (no price delta). To explain why we chose 0.001, there are a couple points to consider. First, any change is very helpful in terms of playing the market through trading, even if it is a small change. Second, a larger threshold could have potentially forced our models to disproportionately predict 0 change (at all time intervals), which would not be a useful model. As we will see below, at larger intervals, our models predict far fewer 0 outcomes, becoming more successful at predicting positive or negative changes in the VIX pricing over time. For each model trial, we first split our data into a train and test set, so that we are later able to assess how well our model performs in both a train set and a not-seen test set. Realistically, training accuracy reflects how well a given model ‘understands’ the data it is presented with, and testing accuracy reflects how well that model can be generalized to accurately form predictions about data it has not yet seen. For each time interval model, we drop the irrelevant time-based price predictors. For example, model30 does not include price_delta_30 or price_delta_60 as predictors. We perform this drop for each time interval model. Baseline Model - Logistic Our baseline model represents a simple logistic regression with a multiclass outcome variable. Using the predictors in our data, the model predicts the 'change in VIX price' classification as positive, negative, or 0 (no change) using a simple logistic regression. # 1 min logreg0 = LogisticRegression(C=10000).fit(X_train0, y_train0) logreg_fit_train0 = logreg0.predict(X_train0) logreg_fit_test0 = logreg0.predict(X_test0) train_scores_logreg0 = accuracy_score(y_train0, logreg_fit_train0) test_scores_logreg0 = accuracy_score(y_test0, logreg_fit_test0) print("Training Accuracy 1 min: ", train_scores_logreg0) print("Testing Accuracy 1 min: ", test_scores_logreg0) # 5 min logreg5 = LogisticRegression(C=10000).fit(X_train5, y_train5) logreg_fit_train5 = logreg5.predict(X_train5) logreg_fit_test5 = logreg5.predict(X_test5) train_scores_logreg5 = accuracy_score(y_train5, logreg_fit_train5) test_scores_logreg5 = accuracy_score(y_test5, logreg_fit_test5) print("Training Accuracy 5 min: ", train_scores_logreg5) print("Testing Accuracy 5 min: ", train_scores_logreg5) # 10 min logreg10 = LogisticRegression(C=10000).fit(X_train10, y_train10) logreg_fit_train10 = logreg10.predict(X_train10) logreg_fit_test10 = logreg10.predict(X_test10) train_scores_logreg10 = accuracy_score(y_train10, logreg_fit_train10) test_scores_logreg10 = accuracy_score(y_test10, logreg_fit_test10) print("Training Accuracy 10 min: ", train_scores_logreg10) print("Testing Accuracy 10 min: ", train_scores_logreg10) # 20 min logreg20 = LogisticRegression(C=10000).fit(X_train20, y_train20) logreg_fit_train20 = logreg20.predict(X_train20) logreg_fit_test20 = logreg20.predict(X_test20) train_scores_logreg20 = accuracy_score(y_train20, logreg_fit_train20) test_scores_logreg20 = accuracy_score(y_test20, logreg_fit_test20) print("Training Accuracy 20 min: ", train_scores_logreg20) print("Testing Accuracy 20 min: ", train_scores_logreg20) # 30 min logreg30 = LogisticRegression(C=10000).fit(X_train30, y_train30) logreg_fit_train30 = logreg30.predict(X_train30) logreg_fit_test30 = logreg30.predict(X_test30) train_scores_logreg30 = accuracy_score(y_train30, logreg_fit_train30) test_scores_logreg30 = accuracy_score(y_test30, logreg_fit_test30) print("Training Accuracy 30 min: ", train_scores_logreg30) print("Testing Accuracy 30 min: ", train_scores_logreg30) # 60 min logreg60 = LogisticRegression(C=10000).fit(X_train60, y_train60) logreg_fit_train60 = logreg60.predict(X_train60) logreg_fit_test60 = logreg60.predict(X_test60) train_scores_logreg60 = accuracy_score(y_train60, logreg_fit_train60) test_scores_logreg60 = accuracy_score(y_test60, logreg_fit_test60) print("Training Accuracy 60 min: ", train_scores_logreg60) print("Testing Accuracy 60 min: ", train_scores_logreg60) Interval  training accuracy   test accuracy 30 minute	0.523760	          0.503306 20 minute	0.515702	          0.500826 60 minute	0.534946	          0.499586 10 minute	0.491529	          0.476033 5 minute	0.476033	          0.452893 1 minute	0.468182	          0.451240 L1 and L2 Regularization We then decided to incorporate regularization in an attempt to improve our logistic model's predictive ability. Lasso regularization (L1) sets the effects/coefficients of unimportant predictors to 0, whereas ridge (L2) simply minimizes/lowers those effects. First, lasso regularization: from sklearn.linear_model import LogisticRegressionCV # lasso lasso = LogisticRegressionCV(cv=5, penalty='l1', max_iter=1000, solver='liblinear') train_scores_logreg_lasso = [] test_scores_logreg_lasso = [] for i in range(len(X_train_list)): lassofit = lasso.fit(X_train_list[i], y_train_list[i]) y_pred_train_lasso = lassofit.predict(X_train_list[i]) y_pred_test_lasso = lassofit.predict(X_test_list[i]) train_score = accuracy_score(y_train_list[i], y_pred_train_lasso) test_score = accuracy_score(y_test_list[i], y_pred_test_lasso) train_scores_logreg_lasso.append(train_score) test_scores_logreg_lasso.append(test_score) print(f'Training set accuracy score for {intervals[i]} using CV & LASSO penalty: {train_score:.4f}') print(f'Test set accuracy score for {intervals[i]} using CV & LASSO penalty: {test_score:.4f}') Interval	training accuracy	test accuracy 1 minute	0.467769	          0.457025 5 minute	0.457025	          0.455372 60 minute	0.444582	          0.419355 30 minute	0.439669	          0.416529 20 minute	0.373140	          0.348760 10 minute	0.217149	          0.200826 Now, ridge regularization: # ridge ridge = LogisticRegressionCV(cv=5, penalty='l2', max_iter=1000, solver='liblinear') train_scores_logreg_ridge = [] test_scores_logreg_ridge = [] for i in range(len(X_train_list)): ridgefit = ridge.fit(X_train_list[i], y_train_list[i]) y_pred_train_ridge = ridgefit.predict(X_train_list[i]) y_pred_test_ridge = ridgefit.predict(X_test_list[i]) train_score = accuracy_score(y_train_list[i], y_pred_train_ridge) test_score = accuracy_score(y_test_list[i], y_pred_test_ridge) train_scores_logreg_ridge.append(train_score) test_scores_logreg_ridge.append(test_score) print(f'Training set accuracy score for {intervals[i]} using CV & Ridge penalty:: {train_score:.4f}') print(f'Test set accuracy score for {intervals[i]} using CV & Ridge penalty:: {test_score:.4f}') Interval	training accuracy	test accuracy 60 minute	0.544458	          0.516129 20 minute	0.526033	          0.500826 30 minute	0.531612	          0.495868 10 minute	0.495455	          0.480165 1 minute	0.469421	          0.451240 5 minute	0.473140	          0.439669 Lasso Regularization does not perform well, whereas ridge gets us just above 50% accuracy on the test set. This suggests that a few predictors may have significant impact and are being pushed to 0 improperly in lasso. Random Forest Our first ensemble method is random forest, which randomly subsets predictors upon which to generate decision trees. We tested out a few different tree depth and number parameters ourselves and determined that a depth of 5 and number of trees of 100 was ideal for our analysis. Below is sample code for one of the time interval models. We perform this for each of the models, and the results can bee seen below this code. # Calibrate num trees and tree depth (after having tried different parameters, saw comparable results) n_trees = 100 tree_depth = 5 # create RF models forest_model0 = RandomForestClassifier(n_estimators=n_trees, max_depth=tree_depth, max_features=int(np.sqrt(len(np_X_train0[0])))) forest_model5 = RandomForestClassifier(n_estimators=n_trees, max_depth=tree_depth, max_features=int(np.sqrt(len(np_X_train5[0])))) forest_model10 = RandomForestClassifier(n_estimators=n_trees, max_depth=tree_depth, max_features=int(np.sqrt(len(np_X_train10[0])))) forest_model20 = RandomForestClassifier(n_estimators=n_trees, max_depth=tree_depth, max_features=int(np.sqrt(len(np_X_train20[0])))) forest_model30 = RandomForestClassifier(n_estimators=n_trees, max_depth=tree_depth, max_features=int(np.sqrt(len(np_X_train30[0])))) forest_model60 = RandomForestClassifier(n_estimators=n_trees, max_depth=tree_depth, max_features=int(np.sqrt(len(np_X_train60[0])))) # fit and calculate accuracy forest_model0.fit(np_X_train0, np_y_train0) y_pred0_forest_train = forest_model0.predict(np_X_train0) y_pred0_forest_test = forest_model0.predict(np_X_test0) random_forest_train_score0 = accuracy_score(np_y_train0, y_pred0_forest_train) random_forest_test_score0 = accuracy_score(np_y_test0, y_pred0_forest_test) print(f'The random forest accuracy on the training set: {random_forest_train_score0}') print(f'The random forest accuracy on the test set: {random_forest_test_score0:.4f}') Interval	training accuracy	test accuracy 60 minute	0.620968	          0.545906 20 minute	0.623347	          0.532231 30 minute	0.611777	          0.514050 10 minute	0.635331	          0.508264 1 minute	0.547934	          0.455372 5 minute	0.591116	          0.446281 Random Forest performs decently relative to our original goal in the project (achieving above 50% test accuracy). Boosting Next, we will consider boosting, an iterative approach that might eliminate some more of the error in our trees. Below is sample code for one of the time interval models. We perform this for each of the models, and the results can bee seen below this code. # initialize parameters like for RF, much the same process - tested different ones and saw best/most consistent results with the following estimators_ADA = 40 learning_ADA = 0.01 tree_depth = 10 model_ADA0 = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=tree_depth), learning_rate=learning_ADA, n_estimators=estimators_ADA) model_ADA0.fit(np_X_train0, np_y_train0) y_train_pred_ADA0 = model_ADA0.predict(np_X_train0) y_test_pred_ADA0 = model_ADA0.predict(np_X_test0) ADA_train0 = accuracy_score(np_y_train0, y_train_pred_ADA0) ADA_test0 = accuracy_score(np_y_test0, y_test_pred_ADA0) print(f'The ADABoost accuracy on the training set: {ADA_train0}') print(f'The ADABoost accuracy on the test set: {ADA_test0:.4f}') ADA_train0_staged = list(model_ADA0.staged_score(np_X_train0, np_y_train0)) ADA_test0_staged = list(model_ADA0.staged_score(np_X_test0, np_y_test0)) # define function to abstract process of building plot def baselearner_plt(n, X_train, y_train, X_test, y_test): # AdaBoostClassifier ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=n), n_estimators=30, learning_rate = 0.05) ada.fit(X_train, y_train) # staged_score train to plot ada_predicts_train = list(ada.staged_score(X_train,y_train)) plt.plot(ada_predicts_train, label = "Train"); # staged_score test to plot ada_predicts_test = list(ada.staged_score(X_test,y_test)) plt.plot(ada_predicts_test, label = "Test"); plt.legend(fontsize=14) plt.title("AdaBoost Classifier Accuracy", fontsize=18) plt.ylabel("Accuracy", fontsize=16) plt.xlabel("Num Iterations", fontsize=16) plt.show() print("Maximum test accuracy for depth of "+str(n)+" is "+str(max(ada_predicts_test))+" at "+str(ada_predicts_test.index(max(ada_predicts_test)))+" iterations") for i in range(len(X_train_list)): baselearner_plt(tree_depth, X_train_list[i], y_train_list[i], X_test_list[i], y_test_list[i]) print("Time Interval: ", intervals[i]) png Maximum test accuracy for depth of 10 is 0.4818181818181818 at 2 iterations Time Interval:  1 minute png Maximum test accuracy for depth of 10 is 0.5107438016528926 at 8 iterations Time Interval:  5 minute png Maximum test accuracy for depth of 10 is 0.5958677685950413 at 9 iterations Time Interval:  10 minute png Maximum test accuracy for depth of 10 is 0.6090909090909091 at 21 iterations Time Interval:  20 minute png Maximum test accuracy for depth of 10 is 0.6347107438016529 at 29 iterations Time Interval:  30 minute png Maximum test accuracy for depth of 10 is 0.6683209263854425 at 5 iterations Time Interval:  60 minute Boosting performs relatively well when comparing all the models. Let’s try a NN to see if we can do better, though. Neural Networks Finally, we created an artificial neural network to classify the changes in VIX price. In this model, we shrunk the number of categories to 2, positive and negative, to see how well this baseline NN performs. # prepare model model = models.Sequential() for i in range(5): model.add(tf.keras.layers.Dense(100, activation='relu')) model.add(layers.Dropout(0.3)) model.add(tf.keras.layers.Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # define parameters epochs = 100 batch_size = 12 validation_split = 0.3 # fit model history0 = model.fit(np_X_train0_NN, np_y_train0_NN, validation_split=validation_split, epochs=epochs, batch_size=batch_size, verbose=False) history5 = model.fit(np_X_train5_NN, np_y_train5_NN, validation_split=validation_split, epochs=epochs, batch_size=batch_size, verbose=False) history10 = model.fit(np_X_train10_NN, np_y_train10_NN, validation_split=validation_split, epochs=epochs, batch_size=batch_size, verbose=False) history20 = model.fit(np_X_train20_NN, np_y_train20_NN, validation_split=validation_split, epochs=epochs, batch_size=batch_size, verbose=False) history30 = model.fit(np_X_train30_NN, np_y_train30_NN, validation_split=validation_split, epochs=epochs, batch_size=batch_size, verbose=False) history60 = model.fit(np_X_train60_NN, np_y_train60_NN, validation_split=validation_split, epochs=epochs, batch_size=batch_size, verbose=False) # output relevant information histories = [history0, history5, history10, history20, history30, history60] for i, history in zip(range(len(intervals)), histories): kaggle_train_acc = history.history['accuracy'][-1] val_loss = history.history['val_loss'][-1] val_acc = history.history['val_accuracy'][-1] diff = val_acc - val_loss loss = history.history['loss'][-1] model.summary() print(f'{intervals[i]} \n ModelTraining Accuracy={kaggle_train_acc}, \n Training Loss={loss}, \n Model Validation Accuracy: {val_acc}, \n Model Validation Loss: {val_loss}, \n Difference between Validation Accuracy and Loss: {diff} \n') Interval	training accuracy	val accuracy	val loss 60 minute	0.844609	          0.618194	          0.369464 30 minute	0.843566	          0.606749	          0.376075 20 minute	0.826741	          0.583333	          0.402431 1 minute	0.789256	          0.573691	          0.426097 10 minute	0.817001	          0.570248	          0.413901 5 minute	0.784829	          0.554408	          0.442316 Even after changing hyperparameters, our neural network does performs pretty much in line with our other models and slightly below boosting. Model Selection Based upon the presented analysis, we conclude that our boosted decision tree classifier at the 60 min time interval, at a depth of 10 with 5 iterations, is the best model. It achieves the highest accuracy in the test set, of 66.8%.",
    "url": "http://localhost:4000/final_notebook/models.html",
    "relUrl": "/final_notebook/models.html"
  }
}
